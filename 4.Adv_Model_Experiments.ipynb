{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef9860e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pnagaraj/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pytorch_lightning import LightningModule\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from adv_train import AdversarialTrainingModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87d8ccaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation and normalization\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
    "])\n",
    "\n",
    "testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=512, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb5dba91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd5a1ae",
   "metadata": {},
   "source": [
    "### Load the Robust Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccdcfbff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model from best_resnet18_cifar100_untargeted_adv.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AdversarialTrainingModule(\n",
       "  (model): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=512, out_features=100, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "version = \"version_2\"  # Best performing version from logs\n",
    "checkpoint_path = f\"lightning_logs/{version}/checkpoints/best-robust.ckpt\"\n",
    "\n",
    "# Load model (automatically handles device placement)\n",
    "model = AdversarialTrainingModule.load_from_checkpoint(checkpoint_path)\n",
    "model.eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b4fa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def improved_pgd_attack(model, images, labels, target_labels=None, epsilon=20/255, alpha=8/255, \n",
    "                       iters=100, random_start=True, untargeted=False):\n",
    "    \"\"\"\n",
    "    Enhanced PGD attack with better performance.\n",
    "    Works with any input tensor shape.\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    images = images.clone().detach().to(device)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    if target_labels is not None:\n",
    "        target_labels = target_labels.to(device)\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Initialize adversarial images\n",
    "    adv_images = images.clone().detach()\n",
    "    \n",
    "    # Start with random noise within epsilon ball if requested\n",
    "    if random_start:\n",
    "        noise = torch.FloatTensor(images.shape).uniform_(-epsilon, epsilon).to(device)\n",
    "        adv_images = adv_images + noise\n",
    "        adv_images = torch.clamp(adv_images, 0, 1)\n",
    "    \n",
    "    best_adv_images = adv_images.clone()\n",
    "    best_loss = None\n",
    "    \n",
    "    for i in range(iters):\n",
    "        adv_images.requires_grad = True\n",
    "        \n",
    "        outputs = model(adv_images)\n",
    "        \n",
    "        # For untargeted attack, maximize loss on true label\n",
    "        # For targeted attack, minimize loss on target label\n",
    "        if untargeted:\n",
    "            loss = -criterion(outputs, labels)\n",
    "        else:\n",
    "            if target_labels is None:\n",
    "                raise ValueError(\"Target labels must be provided for targeted attack\")\n",
    "            loss = criterion(outputs, target_labels)\n",
    "        \n",
    "        # Keep track of best attack images\n",
    "        with torch.no_grad():\n",
    "            if untargeted:\n",
    "                # For untargeted, we want to maximize the loss\n",
    "                _, predicted = outputs.max(1)\n",
    "                success_mask = (predicted != labels)\n",
    "                \n",
    "                # If seeing this success for the first time, save it\n",
    "                if best_loss is None:\n",
    "                    best_loss = -loss\n",
    "                    best_adv_images = adv_images.clone()\n",
    "                else:\n",
    "                    # Update best loss where the new loss is better\n",
    "                    improved_loss = -loss < best_loss\n",
    "                    \n",
    "                    # Create broadcasting mask based on actual tensor dimensions\n",
    "                    # This will work for any shape of input tensors\n",
    "                    batch_size = images.shape[0]\n",
    "                    update_indices = torch.nonzero(improved_loss & success_mask).view(-1)\n",
    "                    \n",
    "                    if len(update_indices) > 0:\n",
    "                        best_adv_images[update_indices] = adv_images[update_indices]\n",
    "                        best_loss[update_indices] = -loss[update_indices]\n",
    "            else:\n",
    "                # For targeted, we want to minimize the loss\n",
    "                _, predicted = outputs.max(1)\n",
    "                success_mask = (predicted == target_labels)\n",
    "                \n",
    "                # If seeing this success for the first time, save it\n",
    "                if best_loss is None:\n",
    "                    best_loss = loss\n",
    "                    best_adv_images = adv_images.clone()\n",
    "                else:\n",
    "                    # Update best loss where the new loss is better\n",
    "                    improved_loss = loss < best_loss\n",
    "                    \n",
    "                    # Create broadcasting mask based on actual tensor dimensions\n",
    "                    batch_size = images.shape[0]\n",
    "                    update_indices = torch.nonzero(improved_loss & success_mask).view(-1)\n",
    "                    \n",
    "                    if len(update_indices) > 0:\n",
    "                        best_adv_images[update_indices] = adv_images[update_indices]\n",
    "                        best_loss[update_indices] = loss[update_indices]\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Get and process gradients\n",
    "        with torch.no_grad():\n",
    "            grad = adv_images.grad.sign()\n",
    "            adv_images = adv_images.detach() - alpha * grad  # For both untargeted and targeted\n",
    "            \n",
    "            # Project back to epsilon ball and valid image range\n",
    "            delta = torch.clamp(adv_images - images, min=-epsilon, max=epsilon)\n",
    "            adv_images = torch.clamp(images + delta, min=0, max=1)\n",
    "\n",
    "            # Optional: Print loss every 10 iterations\n",
    "            if i % 10 == 0:\n",
    "                print(f\"Iteration {i}, Loss: {loss.item():.4f}\")\n",
    "        \n",
    "    return best_adv_images.detach()\n",
    "\n",
    "def improved_evaluate_attack(model, testloader, epsilon=20/255, alpha=8/255, iters=100, subset_size=None, targeted=False):\n",
    "    \"\"\"\n",
    "    Evaluates the model on clean and adversarial examples with improved attack\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    clean_correct = 0\n",
    "    adv_correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Handle subset processing\n",
    "    if subset_size:\n",
    "        try:\n",
    "            subset_loader = torch.utils.data.DataLoader(\n",
    "                testset, batch_size=128, shuffle=True, num_workers=4\n",
    "            )\n",
    "        except NameError:\n",
    "            subset_loader = testloader\n",
    "        max_batches = subset_size // 128 + 1\n",
    "    else:\n",
    "        subset_loader = testloader\n",
    "        max_batches = len(testloader)\n",
    "    \n",
    "    for i, (images, labels) in enumerate(subset_loader):\n",
    "        if i >= max_batches:\n",
    "            break\n",
    "            \n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        # Clean accuracy\n",
    "        with torch.no_grad():\n",
    "            outputs = model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            clean_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        if targeted:\n",
    "            targeted_labels = (labels + 1) % 100\n",
    "        \n",
    "            # Generate adversarial examples using untargeted attack (more effective)\n",
    "            adv_images = improved_pgd_attack(\n",
    "                model, \n",
    "                images, \n",
    "                labels,\n",
    "                target_labels=targeted_labels, \n",
    "                epsilon=epsilon, \n",
    "                alpha=alpha, \n",
    "                iters=iters,\n",
    "                random_start=True,\n",
    "                untargeted=False\n",
    "            )\n",
    "        else:\n",
    "            # Generate adversarial examples using untargeted attack\n",
    "            adv_images = improved_pgd_attack(\n",
    "                model, \n",
    "                images, \n",
    "                labels,\n",
    "                epsilon=epsilon, \n",
    "                alpha=alpha, \n",
    "                iters=iters,\n",
    "                random_start=True,\n",
    "                untargeted=True\n",
    "            )\n",
    "        \n",
    "        # Evaluate adversarial accuracy\n",
    "        with torch.no_grad():\n",
    "            adv_outputs = model(adv_images)\n",
    "            _, adv_predicted = adv_outputs.max(1)\n",
    "            adv_success = (adv_predicted != labels).sum().item()  # Count misclassifications\n",
    "            adv_correct += adv_success\n",
    "    \n",
    "    clean_acc = 100. * clean_correct / total\n",
    "    attack_success = 100. * adv_correct / total\n",
    "    print(f\"Clean Accuracy: {clean_acc:.2f}%\")\n",
    "    print(f\"Attack Success Rate: {attack_success:.2f}%\")\n",
    "    return clean_acc, attack_success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "418e1548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: -2.2824\n",
      "Iteration 10, Loss: -3.9756\n",
      "Iteration 20, Loss: -4.0301\n",
      "Iteration 30, Loss: -4.0415\n",
      "Iteration 40, Loss: -4.0463\n",
      "Iteration 50, Loss: -4.0488\n",
      "Iteration 60, Loss: -4.0503\n",
      "Iteration 70, Loss: -4.0518\n",
      "Iteration 80, Loss: -4.0525\n",
      "Iteration 90, Loss: -4.0536\n",
      "Iteration 0, Loss: -2.2734\n",
      "Iteration 10, Loss: -3.9884\n",
      "Iteration 20, Loss: -4.0401\n",
      "Iteration 30, Loss: -4.0506\n",
      "Iteration 40, Loss: -4.0549\n",
      "Iteration 50, Loss: -4.0574\n",
      "Iteration 60, Loss: -4.0584\n",
      "Iteration 70, Loss: -4.0593\n",
      "Iteration 80, Loss: -4.0607\n",
      "Iteration 90, Loss: -4.0615\n",
      "Iteration 0, Loss: -2.2359\n",
      "Iteration 10, Loss: -3.9452\n",
      "Iteration 20, Loss: -3.9974\n",
      "Iteration 30, Loss: -4.0074\n",
      "Iteration 40, Loss: -4.0120\n",
      "Iteration 50, Loss: -4.0152\n",
      "Iteration 60, Loss: -4.0170\n",
      "Iteration 70, Loss: -4.0182\n",
      "Iteration 80, Loss: -4.0197\n",
      "Iteration 90, Loss: -4.0201\n",
      "Iteration 0, Loss: -2.1582\n",
      "Iteration 10, Loss: -3.8132\n",
      "Iteration 20, Loss: -3.8629\n",
      "Iteration 30, Loss: -3.8736\n",
      "Iteration 40, Loss: -3.8783\n",
      "Iteration 50, Loss: -3.8813\n",
      "Iteration 60, Loss: -3.8832\n",
      "Iteration 70, Loss: -3.8846\n",
      "Iteration 80, Loss: -3.8854\n",
      "Iteration 90, Loss: -3.8863\n",
      "Iteration 0, Loss: -2.2282\n",
      "Iteration 10, Loss: -3.8836\n",
      "Iteration 20, Loss: -3.9376\n",
      "Iteration 30, Loss: -3.9488\n",
      "Iteration 40, Loss: -3.9548\n",
      "Iteration 50, Loss: -3.9578\n",
      "Iteration 60, Loss: -3.9596\n",
      "Iteration 70, Loss: -3.9611\n",
      "Iteration 80, Loss: -3.9620\n",
      "Iteration 90, Loss: -3.9629\n",
      "Iteration 0, Loss: -2.3236\n",
      "Iteration 10, Loss: -4.0216\n",
      "Iteration 20, Loss: -4.0726\n",
      "Iteration 30, Loss: -4.0831\n",
      "Iteration 40, Loss: -4.0880\n",
      "Iteration 50, Loss: -4.0908\n",
      "Iteration 60, Loss: -4.0923\n",
      "Iteration 70, Loss: -4.0928\n",
      "Iteration 80, Loss: -4.0933\n",
      "Iteration 90, Loss: -4.0947\n",
      "Iteration 0, Loss: -2.2944\n",
      "Iteration 10, Loss: -3.9860\n",
      "Iteration 20, Loss: -4.0383\n",
      "Iteration 30, Loss: -4.0499\n",
      "Iteration 40, Loss: -4.0545\n",
      "Iteration 50, Loss: -4.0585\n",
      "Iteration 60, Loss: -4.0599\n",
      "Iteration 70, Loss: -4.0611\n",
      "Iteration 80, Loss: -4.0621\n",
      "Iteration 90, Loss: -4.0631\n",
      "Iteration 0, Loss: -2.1670\n",
      "Iteration 10, Loss: -3.8655\n",
      "Iteration 20, Loss: -3.9193\n",
      "Iteration 30, Loss: -3.9293\n",
      "Iteration 40, Loss: -3.9335\n",
      "Iteration 50, Loss: -3.9360\n",
      "Iteration 60, Loss: -3.9375\n",
      "Iteration 70, Loss: -3.9383\n",
      "Iteration 80, Loss: -3.9392\n",
      "Iteration 90, Loss: -3.9403\n",
      "Iteration 0, Loss: -2.2502\n",
      "Iteration 10, Loss: -3.9487\n",
      "Iteration 20, Loss: -3.9988\n",
      "Iteration 30, Loss: -4.0088\n",
      "Iteration 40, Loss: -4.0135\n",
      "Iteration 50, Loss: -4.0166\n",
      "Iteration 60, Loss: -4.0189\n",
      "Iteration 70, Loss: -4.0200\n",
      "Iteration 80, Loss: -4.0204\n",
      "Iteration 90, Loss: -4.0218\n",
      "Iteration 0, Loss: -2.3709\n",
      "Iteration 10, Loss: -4.1462\n",
      "Iteration 20, Loss: -4.2013\n",
      "Iteration 30, Loss: -4.2135\n",
      "Iteration 40, Loss: -4.2193\n",
      "Iteration 50, Loss: -4.2217\n",
      "Iteration 60, Loss: -4.2240\n",
      "Iteration 70, Loss: -4.2258\n",
      "Iteration 80, Loss: -4.2273\n",
      "Iteration 90, Loss: -4.2277\n",
      "Iteration 0, Loss: -2.3135\n",
      "Iteration 10, Loss: -4.0475\n",
      "Iteration 20, Loss: -4.1018\n",
      "Iteration 30, Loss: -4.1137\n",
      "Iteration 40, Loss: -4.1186\n",
      "Iteration 50, Loss: -4.1215\n",
      "Iteration 60, Loss: -4.1237\n",
      "Iteration 70, Loss: -4.1247\n",
      "Iteration 80, Loss: -4.1262\n",
      "Iteration 90, Loss: -4.1267\n",
      "Iteration 0, Loss: -2.3025\n",
      "Iteration 10, Loss: -3.9764\n",
      "Iteration 20, Loss: -4.0241\n",
      "Iteration 30, Loss: -4.0339\n",
      "Iteration 40, Loss: -4.0380\n",
      "Iteration 50, Loss: -4.0409\n",
      "Iteration 60, Loss: -4.0423\n",
      "Iteration 70, Loss: -4.0436\n",
      "Iteration 80, Loss: -4.0441\n",
      "Iteration 90, Loss: -4.0448\n",
      "Iteration 0, Loss: -2.2915\n",
      "Iteration 10, Loss: -3.9843\n",
      "Iteration 20, Loss: -4.0346\n",
      "Iteration 30, Loss: -4.0451\n",
      "Iteration 40, Loss: -4.0500\n",
      "Iteration 50, Loss: -4.0527\n",
      "Iteration 60, Loss: -4.0546\n",
      "Iteration 70, Loss: -4.0562\n",
      "Iteration 80, Loss: -4.0575\n",
      "Iteration 90, Loss: -4.0588\n",
      "Iteration 0, Loss: -2.2604\n",
      "Iteration 10, Loss: -3.9726\n",
      "Iteration 20, Loss: -4.0244\n",
      "Iteration 30, Loss: -4.0351\n",
      "Iteration 40, Loss: -4.0392\n",
      "Iteration 50, Loss: -4.0413\n",
      "Iteration 60, Loss: -4.0423\n",
      "Iteration 70, Loss: -4.0440\n",
      "Iteration 80, Loss: -4.0447\n",
      "Iteration 90, Loss: -4.0454\n",
      "Iteration 0, Loss: -2.3655\n",
      "Iteration 10, Loss: -4.0249\n",
      "Iteration 20, Loss: -4.0716\n",
      "Iteration 30, Loss: -4.0813\n",
      "Iteration 40, Loss: -4.0859\n",
      "Iteration 50, Loss: -4.0883\n",
      "Iteration 60, Loss: -4.0906\n",
      "Iteration 70, Loss: -4.0914\n",
      "Iteration 80, Loss: -4.0922\n",
      "Iteration 90, Loss: -4.0930\n",
      "Iteration 0, Loss: -2.4077\n",
      "Iteration 10, Loss: -4.1033\n",
      "Iteration 20, Loss: -4.1559\n",
      "Iteration 30, Loss: -4.1664\n",
      "Iteration 40, Loss: -4.1705\n",
      "Iteration 50, Loss: -4.1735\n",
      "Iteration 60, Loss: -4.1750\n",
      "Iteration 70, Loss: -4.1761\n",
      "Iteration 80, Loss: -4.1769\n",
      "Iteration 90, Loss: -4.1777\n",
      "Iteration 0, Loss: -2.1712\n",
      "Iteration 10, Loss: -3.8708\n",
      "Iteration 20, Loss: -3.9226\n",
      "Iteration 30, Loss: -3.9334\n",
      "Iteration 40, Loss: -3.9379\n",
      "Iteration 50, Loss: -3.9411\n",
      "Iteration 60, Loss: -3.9427\n",
      "Iteration 70, Loss: -3.9437\n",
      "Iteration 80, Loss: -3.9450\n",
      "Iteration 90, Loss: -3.9453\n",
      "Iteration 0, Loss: -2.2962\n",
      "Iteration 10, Loss: -3.9755\n",
      "Iteration 20, Loss: -4.0271\n",
      "Iteration 30, Loss: -4.0366\n",
      "Iteration 40, Loss: -4.0403\n",
      "Iteration 50, Loss: -4.0431\n",
      "Iteration 60, Loss: -4.0450\n",
      "Iteration 70, Loss: -4.0466\n",
      "Iteration 80, Loss: -4.0475\n",
      "Iteration 90, Loss: -4.0479\n",
      "Iteration 0, Loss: -2.1609\n",
      "Iteration 10, Loss: -3.8329\n",
      "Iteration 20, Loss: -3.8857\n",
      "Iteration 30, Loss: -3.8972\n",
      "Iteration 40, Loss: -3.9016\n",
      "Iteration 50, Loss: -3.9048\n",
      "Iteration 60, Loss: -3.9069\n",
      "Iteration 70, Loss: -3.9079\n",
      "Iteration 80, Loss: -3.9093\n",
      "Iteration 90, Loss: -3.9101\n",
      "Iteration 0, Loss: -2.2558\n",
      "Iteration 10, Loss: -3.9352\n",
      "Iteration 20, Loss: -3.9850\n",
      "Iteration 30, Loss: -3.9945\n",
      "Iteration 40, Loss: -3.9986\n",
      "Iteration 50, Loss: -4.0014\n",
      "Iteration 60, Loss: -4.0022\n",
      "Iteration 70, Loss: -4.0039\n",
      "Iteration 80, Loss: -4.0043\n",
      "Iteration 90, Loss: -4.0055\n",
      "Clean Accuracy: 45.36%\n",
      "Attack Success Rate: 55.44%\n"
     ]
    }
   ],
   "source": [
    "clean_acc, attack_success = improved_evaluate_attack(\n",
    "    model, \n",
    "    test_loader, \n",
    "    epsilon=30/255,  # Increased perturbation budget\n",
    "    alpha=8/255,     # Same step size\n",
    "    iters=100,       # More iterations\n",
    "    subset_size=None # Optional: limit evaluation size for faster results\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5ec69d",
   "metadata": {},
   "source": [
    "### Clean Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ff78222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model from best_resnet18_cifar100_untargeted_adv.pth\n",
      "Iteration 0, Loss: -2.7517\n",
      "Iteration 10, Loss: -3.7664\n",
      "Iteration 20, Loss: -3.7928\n",
      "Iteration 30, Loss: -3.7987\n",
      "Iteration 40, Loss: -3.8014\n",
      "Iteration 50, Loss: -3.8028\n",
      "Iteration 60, Loss: -3.8039\n",
      "Iteration 70, Loss: -3.8046\n",
      "Iteration 80, Loss: -3.8052\n",
      "Iteration 90, Loss: -3.8055\n",
      "Iteration 0, Loss: -2.8129\n",
      "Iteration 10, Loss: -3.8217\n",
      "Iteration 20, Loss: -3.8455\n",
      "Iteration 30, Loss: -3.8502\n",
      "Iteration 40, Loss: -3.8525\n",
      "Iteration 50, Loss: -3.8536\n",
      "Iteration 60, Loss: -3.8544\n",
      "Iteration 70, Loss: -3.8550\n",
      "Iteration 80, Loss: -3.8553\n",
      "Iteration 90, Loss: -3.8556\n",
      "Iteration 0, Loss: -2.7891\n",
      "Iteration 10, Loss: -3.7821\n",
      "Iteration 20, Loss: -3.8061\n",
      "Iteration 30, Loss: -3.8111\n",
      "Iteration 40, Loss: -3.8130\n",
      "Iteration 50, Loss: -3.8142\n",
      "Iteration 60, Loss: -3.8151\n",
      "Iteration 70, Loss: -3.8157\n",
      "Iteration 80, Loss: -3.8160\n",
      "Iteration 90, Loss: -3.8163\n",
      "Iteration 0, Loss: -2.7723\n",
      "Iteration 10, Loss: -3.7532\n",
      "Iteration 20, Loss: -3.7773\n",
      "Iteration 30, Loss: -3.7824\n",
      "Iteration 40, Loss: -3.7847\n",
      "Iteration 50, Loss: -3.7859\n",
      "Iteration 60, Loss: -3.7869\n",
      "Iteration 70, Loss: -3.7874\n",
      "Iteration 80, Loss: -3.7878\n",
      "Iteration 90, Loss: -3.7881\n",
      "Iteration 0, Loss: -2.7896\n",
      "Iteration 10, Loss: -3.7898\n",
      "Iteration 20, Loss: -3.8155\n",
      "Iteration 30, Loss: -3.8209\n",
      "Iteration 40, Loss: -3.8235\n",
      "Iteration 50, Loss: -3.8249\n",
      "Iteration 60, Loss: -3.8259\n",
      "Iteration 70, Loss: -3.8265\n",
      "Iteration 80, Loss: -3.8271\n",
      "Iteration 90, Loss: -3.8274\n",
      "Iteration 0, Loss: -2.8225\n",
      "Iteration 10, Loss: -3.7853\n",
      "Iteration 20, Loss: -3.8075\n",
      "Iteration 30, Loss: -3.8119\n",
      "Iteration 40, Loss: -3.8139\n",
      "Iteration 50, Loss: -3.8151\n",
      "Iteration 60, Loss: -3.8159\n",
      "Iteration 70, Loss: -3.8164\n",
      "Iteration 80, Loss: -3.8169\n",
      "Iteration 90, Loss: -3.8172\n",
      "Iteration 0, Loss: -2.8163\n",
      "Iteration 10, Loss: -3.8255\n",
      "Iteration 20, Loss: -3.8511\n",
      "Iteration 30, Loss: -3.8562\n",
      "Iteration 40, Loss: -3.8586\n",
      "Iteration 50, Loss: -3.8600\n",
      "Iteration 60, Loss: -3.8610\n",
      "Iteration 70, Loss: -3.8617\n",
      "Iteration 80, Loss: -3.8621\n",
      "Iteration 90, Loss: -3.8623\n",
      "Iteration 0, Loss: -2.7962\n",
      "Iteration 10, Loss: -3.8092\n",
      "Iteration 20, Loss: -3.8337\n",
      "Iteration 30, Loss: -3.8390\n",
      "Iteration 40, Loss: -3.8412\n",
      "Iteration 50, Loss: -3.8425\n",
      "Iteration 60, Loss: -3.8434\n",
      "Iteration 70, Loss: -3.8440\n",
      "Iteration 80, Loss: -3.8444\n",
      "Iteration 90, Loss: -3.8449\n",
      "Iteration 0, Loss: -2.8539\n",
      "Iteration 10, Loss: -3.8421\n",
      "Iteration 20, Loss: -3.8655\n",
      "Iteration 30, Loss: -3.8701\n",
      "Iteration 40, Loss: -3.8721\n",
      "Iteration 50, Loss: -3.8732\n",
      "Iteration 60, Loss: -3.8739\n",
      "Iteration 70, Loss: -3.8744\n",
      "Iteration 80, Loss: -3.8747\n",
      "Iteration 90, Loss: -3.8750\n",
      "Iteration 0, Loss: -2.9280\n",
      "Iteration 10, Loss: -3.9852\n",
      "Iteration 20, Loss: -4.0129\n",
      "Iteration 30, Loss: -4.0189\n",
      "Iteration 40, Loss: -4.0214\n",
      "Iteration 50, Loss: -4.0230\n",
      "Iteration 60, Loss: -4.0239\n",
      "Iteration 70, Loss: -4.0246\n",
      "Iteration 80, Loss: -4.0251\n",
      "Iteration 90, Loss: -4.0254\n",
      "Iteration 0, Loss: -2.8238\n",
      "Iteration 10, Loss: -3.8404\n",
      "Iteration 20, Loss: -3.8654\n",
      "Iteration 30, Loss: -3.8703\n",
      "Iteration 40, Loss: -3.8723\n",
      "Iteration 50, Loss: -3.8734\n",
      "Iteration 60, Loss: -3.8744\n",
      "Iteration 70, Loss: -3.8748\n",
      "Iteration 80, Loss: -3.8753\n",
      "Iteration 90, Loss: -3.8756\n",
      "Iteration 0, Loss: -2.8118\n",
      "Iteration 10, Loss: -3.8120\n",
      "Iteration 20, Loss: -3.8367\n",
      "Iteration 30, Loss: -3.8418\n",
      "Iteration 40, Loss: -3.8438\n",
      "Iteration 50, Loss: -3.8449\n",
      "Iteration 60, Loss: -3.8458\n",
      "Iteration 70, Loss: -3.8464\n",
      "Iteration 80, Loss: -3.8469\n",
      "Iteration 90, Loss: -3.8473\n",
      "Iteration 0, Loss: -2.8174\n",
      "Iteration 10, Loss: -3.8103\n",
      "Iteration 20, Loss: -3.8348\n",
      "Iteration 30, Loss: -3.8398\n",
      "Iteration 40, Loss: -3.8419\n",
      "Iteration 50, Loss: -3.8432\n",
      "Iteration 60, Loss: -3.8441\n",
      "Iteration 70, Loss: -3.8447\n",
      "Iteration 80, Loss: -3.8452\n",
      "Iteration 90, Loss: -3.8456\n",
      "Iteration 0, Loss: -2.8378\n",
      "Iteration 10, Loss: -3.8346\n",
      "Iteration 20, Loss: -3.8585\n",
      "Iteration 30, Loss: -3.8638\n",
      "Iteration 40, Loss: -3.8659\n",
      "Iteration 50, Loss: -3.8671\n",
      "Iteration 60, Loss: -3.8679\n",
      "Iteration 70, Loss: -3.8686\n",
      "Iteration 80, Loss: -3.8690\n",
      "Iteration 90, Loss: -3.8694\n",
      "Iteration 0, Loss: -2.8811\n",
      "Iteration 10, Loss: -3.8380\n",
      "Iteration 20, Loss: -3.8595\n",
      "Iteration 30, Loss: -3.8636\n",
      "Iteration 40, Loss: -3.8656\n",
      "Iteration 50, Loss: -3.8667\n",
      "Iteration 60, Loss: -3.8675\n",
      "Iteration 70, Loss: -3.8679\n",
      "Iteration 80, Loss: -3.8683\n",
      "Iteration 90, Loss: -3.8687\n",
      "Iteration 0, Loss: -2.9282\n",
      "Iteration 10, Loss: -3.8953\n",
      "Iteration 20, Loss: -3.9184\n",
      "Iteration 30, Loss: -3.9232\n",
      "Iteration 40, Loss: -3.9253\n",
      "Iteration 50, Loss: -3.9264\n",
      "Iteration 60, Loss: -3.9271\n",
      "Iteration 70, Loss: -3.9278\n",
      "Iteration 80, Loss: -3.9281\n",
      "Iteration 90, Loss: -3.9285\n",
      "Iteration 0, Loss: -2.7666\n",
      "Iteration 10, Loss: -3.7684\n",
      "Iteration 20, Loss: -3.7931\n",
      "Iteration 30, Loss: -3.7981\n",
      "Iteration 40, Loss: -3.8002\n",
      "Iteration 50, Loss: -3.8014\n",
      "Iteration 60, Loss: -3.8022\n",
      "Iteration 70, Loss: -3.8028\n",
      "Iteration 80, Loss: -3.8032\n",
      "Iteration 90, Loss: -3.8035\n",
      "Iteration 0, Loss: -2.8312\n",
      "Iteration 10, Loss: -3.8278\n",
      "Iteration 20, Loss: -3.8531\n",
      "Iteration 30, Loss: -3.8581\n",
      "Iteration 40, Loss: -3.8605\n",
      "Iteration 50, Loss: -3.8620\n",
      "Iteration 60, Loss: -3.8628\n",
      "Iteration 70, Loss: -3.8635\n",
      "Iteration 80, Loss: -3.8641\n",
      "Iteration 90, Loss: -3.8644\n",
      "Iteration 0, Loss: -2.7438\n",
      "Iteration 10, Loss: -3.7595\n",
      "Iteration 20, Loss: -3.7849\n",
      "Iteration 30, Loss: -3.7903\n",
      "Iteration 40, Loss: -3.7928\n",
      "Iteration 50, Loss: -3.7942\n",
      "Iteration 60, Loss: -3.7952\n",
      "Iteration 70, Loss: -3.7958\n",
      "Iteration 80, Loss: -3.7964\n",
      "Iteration 90, Loss: -3.7968\n",
      "Iteration 0, Loss: -2.8409\n",
      "Iteration 10, Loss: -3.8189\n",
      "Iteration 20, Loss: -3.8420\n",
      "Iteration 30, Loss: -3.8466\n",
      "Iteration 40, Loss: -3.8481\n",
      "Iteration 50, Loss: -3.8492\n",
      "Iteration 60, Loss: -3.8500\n",
      "Iteration 70, Loss: -3.8507\n",
      "Iteration 80, Loss: -3.8510\n",
      "Iteration 90, Loss: -3.8516\n",
      "Clean Accuracy: 47.33%\n",
      "Attack Success Rate: 60.88%\n"
     ]
    }
   ],
   "source": [
    "version = \"version_2\"  # Best performing version from logs\n",
    "checkpoint_path = f\"lightning_logs/{version}/checkpoints/best-clean.ckpt\"\n",
    "\n",
    "# Load model (automatically handles device placement)\n",
    "model = AdversarialTrainingModule.load_from_checkpoint(checkpoint_path)\n",
    "model.eval().to(device)\n",
    "\n",
    "clean_acc, attack_success = improved_evaluate_attack(\n",
    "    model, \n",
    "    test_loader, \n",
    "    epsilon=30/255,  # Increased perturbation budget\n",
    "    alpha=8/255,     # Same step size\n",
    "    iters=100,       # More iterations\n",
    "    subset_size=None # Optional: limit evaluation size for faster results\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8425b9ec",
   "metadata": {},
   "source": [
    "### Targeted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae9925bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pgd_attack(model, images, labels, target_labels, epsilon=16/255, alpha=4/255, iters=40):\n",
    "    \"\"\"\n",
    "    PGD attack with proper gradient tracking.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to attack\n",
    "        images: Clean images\n",
    "        labels: True labels\n",
    "        target_labels: Target labels for the attack\n",
    "        epsilon: Maximum perturbation\n",
    "        alpha: Step size\n",
    "        iters: Number of iterations\n",
    "        \n",
    "    Returns:\n",
    "        Adversarial images\n",
    "    \"\"\"\n",
    "    # Ensure we're working with a device-consistent copy\n",
    "    device = next(model.parameters()).device\n",
    "    images = images.clone().detach().to(device)\n",
    "    labels = labels.to(device)\n",
    "    target_labels = target_labels.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Initialize adversarial images\n",
    "    adv_images = images.clone().detach()\n",
    "\n",
    "    for i in range(iters):\n",
    "        # Important: Create a fresh copy that requires gradients\n",
    "        adv_images = adv_images.detach().requires_grad_(True)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(adv_images)\n",
    "        loss = criterion(outputs, target_labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Get gradients\n",
    "        grad = adv_images.grad.detach()\n",
    "        \n",
    "        # Update adversarial images\n",
    "        with torch.no_grad():\n",
    "            adv_images = adv_images - alpha * grad.sign()  # Perturb toward target\n",
    "            delta = torch.clamp(adv_images - images, min=-epsilon, max=epsilon)\n",
    "            adv_images = torch.clamp(images + delta, min=0, max=1)\n",
    "\n",
    "        # Optional debugging\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Iteration {i}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    return adv_images.detach()\n",
    "\n",
    "def evaluate_full_test(model, testloader, epsilon=16/255, alpha=4/255, iters=20, subset_size=None):\n",
    "    \"\"\"\n",
    "    Evaluates the model on clean and adversarial examples.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    clean_correct = 0\n",
    "    adv_correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # If subset_size is specified, limit the evaluation\n",
    "    if subset_size:\n",
    "        # Make sure testset is defined before using it\n",
    "        # If not defined, use a subset of testloader\n",
    "        try:\n",
    "            subset_loader = torch.utils.data.DataLoader(\n",
    "                testset, batch_size=256, shuffle=True, num_workers=4\n",
    "            )\n",
    "        except NameError:\n",
    "            # Create a subset from existing loader\n",
    "            subset_loader = testloader\n",
    "        max_batches = subset_size // 256 + 1\n",
    "    else:\n",
    "        subset_loader = testloader\n",
    "        max_batches = len(testloader)\n",
    "\n",
    "    for i, (images, labels) in enumerate(subset_loader):\n",
    "        if i >= max_batches:\n",
    "            break\n",
    "            \n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        total += labels.size(0)\n",
    "\n",
    "        # Clean accuracy (without gradients)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            clean_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        # Generate adversarial examples (requires gradients)\n",
    "        target_labels = (labels + 1) % 100  # Example: shift to next class\n",
    "        adv_images = pgd_attack(model, images, labels, target_labels, epsilon, alpha, iters)\n",
    "\n",
    "        # Adversarial accuracy (without gradients)\n",
    "        with torch.no_grad():\n",
    "            adv_outputs = model(adv_images)\n",
    "            _, adv_predicted = adv_outputs.max(1)\n",
    "            adv_correct += adv_predicted.eq(target_labels).sum().item()\n",
    "\n",
    "    clean_acc = 100. * clean_correct / total\n",
    "    attack_success = 100. * adv_correct / total\n",
    "    print(f\"Clean Accuracy: {clean_acc:.2f}%\")\n",
    "    print(f\"Attack Success Rate: {attack_success:.2f}%\")\n",
    "    return clean_acc, attack_success"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58fc714",
   "metadata": {},
   "source": [
    "### Robust Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c65afed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 7.0596\n",
      "Iteration 10, Loss: 5.6524\n",
      "Iteration 20, Loss: 4.5831\n",
      "Iteration 30, Loss: 4.4306\n",
      "Iteration 40, Loss: 4.3655\n",
      "Iteration 0, Loss: 6.9006\n",
      "Iteration 10, Loss: 5.5137\n",
      "Iteration 20, Loss: 4.4494\n",
      "Iteration 30, Loss: 4.2967\n",
      "Iteration 40, Loss: 4.2317\n",
      "Iteration 0, Loss: 7.2039\n",
      "Iteration 10, Loss: 5.8174\n",
      "Iteration 20, Loss: 4.7493\n",
      "Iteration 30, Loss: 4.5931\n",
      "Iteration 40, Loss: 4.5273\n",
      "Iteration 0, Loss: 6.9577\n",
      "Iteration 10, Loss: 5.6121\n",
      "Iteration 20, Loss: 4.5750\n",
      "Iteration 30, Loss: 4.4313\n",
      "Iteration 40, Loss: 4.3720\n",
      "Iteration 0, Loss: 7.1000\n",
      "Iteration 10, Loss: 5.7015\n",
      "Iteration 20, Loss: 4.6240\n",
      "Iteration 30, Loss: 4.4677\n",
      "Iteration 40, Loss: 4.4019\n",
      "Iteration 0, Loss: 6.9405\n",
      "Iteration 10, Loss: 5.5673\n",
      "Iteration 20, Loss: 4.5056\n",
      "Iteration 30, Loss: 4.3556\n",
      "Iteration 40, Loss: 4.2931\n",
      "Iteration 0, Loss: 6.9901\n",
      "Iteration 10, Loss: 5.6295\n",
      "Iteration 20, Loss: 4.5942\n",
      "Iteration 30, Loss: 4.4444\n",
      "Iteration 40, Loss: 4.3813\n",
      "Iteration 0, Loss: 7.2045\n",
      "Iteration 10, Loss: 5.7986\n",
      "Iteration 20, Loss: 4.7113\n",
      "Iteration 30, Loss: 4.5545\n",
      "Iteration 40, Loss: 4.4876\n",
      "Iteration 0, Loss: 7.0526\n",
      "Iteration 10, Loss: 5.6904\n",
      "Iteration 20, Loss: 4.6478\n",
      "Iteration 30, Loss: 4.4931\n",
      "Iteration 40, Loss: 4.4267\n",
      "Iteration 0, Loss: 6.8300\n",
      "Iteration 10, Loss: 5.4547\n",
      "Iteration 20, Loss: 4.3898\n",
      "Iteration 30, Loss: 4.2269\n",
      "Iteration 40, Loss: 4.1573\n",
      "Iteration 0, Loss: 6.9377\n",
      "Iteration 10, Loss: 5.5518\n",
      "Iteration 20, Loss: 4.4943\n",
      "Iteration 30, Loss: 4.3376\n",
      "Iteration 40, Loss: 4.2704\n",
      "Iteration 0, Loss: 7.0346\n",
      "Iteration 10, Loss: 5.6636\n",
      "Iteration 20, Loss: 4.5990\n",
      "Iteration 30, Loss: 4.4445\n",
      "Iteration 40, Loss: 4.3785\n",
      "Iteration 0, Loss: 7.0920\n",
      "Iteration 10, Loss: 5.7109\n",
      "Iteration 20, Loss: 4.6371\n",
      "Iteration 30, Loss: 4.4829\n",
      "Iteration 40, Loss: 4.4180\n",
      "Iteration 0, Loss: 7.0995\n",
      "Iteration 10, Loss: 5.7137\n",
      "Iteration 20, Loss: 4.6584\n",
      "Iteration 30, Loss: 4.5072\n",
      "Iteration 40, Loss: 4.4437\n",
      "Iteration 0, Loss: 7.1070\n",
      "Iteration 10, Loss: 5.7741\n",
      "Iteration 20, Loss: 4.7536\n",
      "Iteration 30, Loss: 4.6059\n",
      "Iteration 40, Loss: 4.5434\n",
      "Iteration 0, Loss: 6.9102\n",
      "Iteration 10, Loss: 5.5605\n",
      "Iteration 20, Loss: 4.5134\n",
      "Iteration 30, Loss: 4.3597\n",
      "Iteration 40, Loss: 4.2948\n",
      "Iteration 0, Loss: 7.0145\n",
      "Iteration 10, Loss: 5.6282\n",
      "Iteration 20, Loss: 4.5604\n",
      "Iteration 30, Loss: 4.4095\n",
      "Iteration 40, Loss: 4.3466\n",
      "Iteration 0, Loss: 7.0671\n",
      "Iteration 10, Loss: 5.7002\n",
      "Iteration 20, Loss: 4.6477\n",
      "Iteration 30, Loss: 4.4952\n",
      "Iteration 40, Loss: 4.4297\n",
      "Iteration 0, Loss: 6.9864\n",
      "Iteration 10, Loss: 5.5739\n",
      "Iteration 20, Loss: 4.4905\n",
      "Iteration 30, Loss: 4.3348\n",
      "Iteration 40, Loss: 4.2686\n",
      "Iteration 0, Loss: 6.9562\n",
      "Iteration 10, Loss: 5.5703\n",
      "Iteration 20, Loss: 4.5287\n",
      "Iteration 30, Loss: 4.3800\n",
      "Iteration 40, Loss: 4.3161\n",
      "Clean Accuracy: 45.36%\n",
      "Attack Success Rate: 21.92%\n"
     ]
    }
   ],
   "source": [
    "clean_acc, attack_success = evaluate_full_test(model, test_loader, epsilon=40/255, alpha=2/255, iters=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc295921",
   "metadata": {},
   "source": [
    "### Best Clean Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ee0bb99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model from best_resnet18_cifar100_untargeted_adv.pth\n",
      "Iteration 0, Loss: 5.7725\n",
      "Iteration 10, Loss: 4.8336\n",
      "Iteration 20, Loss: 4.2479\n",
      "Iteration 30, Loss: 4.1585\n",
      "Iteration 40, Loss: 4.1228\n",
      "Iteration 0, Loss: 5.6736\n",
      "Iteration 10, Loss: 4.7457\n",
      "Iteration 20, Loss: 4.1560\n",
      "Iteration 30, Loss: 4.0660\n",
      "Iteration 40, Loss: 4.0295\n",
      "Iteration 0, Loss: 5.8556\n",
      "Iteration 10, Loss: 4.9175\n",
      "Iteration 20, Loss: 4.3260\n",
      "Iteration 30, Loss: 4.2331\n",
      "Iteration 40, Loss: 4.1953\n",
      "Iteration 0, Loss: 5.6808\n",
      "Iteration 10, Loss: 4.7817\n",
      "Iteration 20, Loss: 4.2244\n",
      "Iteration 30, Loss: 4.1402\n",
      "Iteration 40, Loss: 4.1066\n",
      "Iteration 0, Loss: 5.8200\n",
      "Iteration 10, Loss: 4.8456\n",
      "Iteration 20, Loss: 4.2486\n",
      "Iteration 30, Loss: 4.1567\n",
      "Iteration 40, Loss: 4.1203\n",
      "Iteration 0, Loss: 5.6837\n",
      "Iteration 10, Loss: 4.7740\n",
      "Iteration 20, Loss: 4.1986\n",
      "Iteration 30, Loss: 4.1127\n",
      "Iteration 40, Loss: 4.0791\n",
      "Iteration 0, Loss: 5.6848\n",
      "Iteration 10, Loss: 4.7874\n",
      "Iteration 20, Loss: 4.2159\n",
      "Iteration 30, Loss: 4.1270\n",
      "Iteration 40, Loss: 4.0911\n",
      "Iteration 0, Loss: 5.8321\n",
      "Iteration 10, Loss: 4.9045\n",
      "Iteration 20, Loss: 4.3151\n",
      "Iteration 30, Loss: 4.2221\n",
      "Iteration 40, Loss: 4.1841\n",
      "Iteration 0, Loss: 5.7547\n",
      "Iteration 10, Loss: 4.8229\n",
      "Iteration 20, Loss: 4.2604\n",
      "Iteration 30, Loss: 4.1748\n",
      "Iteration 40, Loss: 4.1407\n",
      "Iteration 0, Loss: 5.6791\n",
      "Iteration 10, Loss: 4.7292\n",
      "Iteration 20, Loss: 4.1222\n",
      "Iteration 30, Loss: 4.0254\n",
      "Iteration 40, Loss: 3.9852\n",
      "Iteration 0, Loss: 5.6750\n",
      "Iteration 10, Loss: 4.7397\n",
      "Iteration 20, Loss: 4.1455\n",
      "Iteration 30, Loss: 4.0539\n",
      "Iteration 40, Loss: 4.0172\n",
      "Iteration 0, Loss: 5.7473\n",
      "Iteration 10, Loss: 4.8307\n",
      "Iteration 20, Loss: 4.2486\n",
      "Iteration 30, Loss: 4.1583\n",
      "Iteration 40, Loss: 4.1211\n",
      "Iteration 0, Loss: 5.7661\n",
      "Iteration 10, Loss: 4.8637\n",
      "Iteration 20, Loss: 4.2860\n",
      "Iteration 30, Loss: 4.1962\n",
      "Iteration 40, Loss: 4.1598\n",
      "Iteration 0, Loss: 5.7772\n",
      "Iteration 10, Loss: 4.8509\n",
      "Iteration 20, Loss: 4.2711\n",
      "Iteration 30, Loss: 4.1819\n",
      "Iteration 40, Loss: 4.1464\n",
      "Iteration 0, Loss: 5.7792\n",
      "Iteration 10, Loss: 4.8806\n",
      "Iteration 20, Loss: 4.3218\n",
      "Iteration 30, Loss: 4.2373\n",
      "Iteration 40, Loss: 4.2037\n",
      "Iteration 0, Loss: 5.6230\n",
      "Iteration 10, Loss: 4.7552\n",
      "Iteration 20, Loss: 4.1945\n",
      "Iteration 30, Loss: 4.1077\n",
      "Iteration 40, Loss: 4.0729\n",
      "Iteration 0, Loss: 5.7383\n",
      "Iteration 10, Loss: 4.8250\n",
      "Iteration 20, Loss: 4.2566\n",
      "Iteration 30, Loss: 4.1708\n",
      "Iteration 40, Loss: 4.1370\n",
      "Iteration 0, Loss: 5.7836\n",
      "Iteration 10, Loss: 4.8320\n",
      "Iteration 20, Loss: 4.2445\n",
      "Iteration 30, Loss: 4.1544\n",
      "Iteration 40, Loss: 4.1180\n",
      "Iteration 0, Loss: 5.6884\n",
      "Iteration 10, Loss: 4.7470\n",
      "Iteration 20, Loss: 4.1503\n",
      "Iteration 30, Loss: 4.0561\n",
      "Iteration 40, Loss: 4.0180\n",
      "Iteration 0, Loss: 5.7591\n",
      "Iteration 10, Loss: 4.8367\n",
      "Iteration 20, Loss: 4.2771\n",
      "Iteration 30, Loss: 4.1906\n",
      "Iteration 40, Loss: 4.1553\n",
      "Clean Accuracy: 47.33%\n",
      "Attack Success Rate: 17.50%\n"
     ]
    }
   ],
   "source": [
    "version = \"version_2\"  # Best performing version from logs\n",
    "checkpoint_path = f\"lightning_logs/{version}/checkpoints/best-clean.ckpt\"\n",
    "\n",
    "# Load model (automatically handles device placement)\n",
    "model = AdversarialTrainingModule.load_from_checkpoint(checkpoint_path)\n",
    "model.eval().to(device)\n",
    "\n",
    "clean_acc, attack_success = evaluate_full_test(model, test_loader, epsilon=40/255, alpha=2/255, iters=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

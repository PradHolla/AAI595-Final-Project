{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba6c644f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torchvision.datasets import CIFAR100\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "332177bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR100DataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir='./data', batch_size=128, num_workers=4):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        \n",
    "        # Data transforms\n",
    "        self.transform_train = transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.AutoAugment(transforms.AutoAugmentPolicy.CIFAR10),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
    "        ])\n",
    "        \n",
    "        self.transform_test = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
    "        ])\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        # Download data if needed\n",
    "        CIFAR100(self.data_dir, train=True, download=True)\n",
    "        CIFAR100(self.data_dir, train=False, download=True)\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        # Load datasets\n",
    "        cifar_full = CIFAR100(self.data_dir, train=True, transform=self.transform_train)\n",
    "        \n",
    "        # Split into train and validation\n",
    "        train_size = int(0.9 * len(cifar_full))\n",
    "        val_size = len(cifar_full) - train_size\n",
    "        self.train_dataset, self.val_dataset = random_split(\n",
    "            cifar_full, [train_size, val_size], \n",
    "            generator=torch.Generator().manual_seed(42)\n",
    "        )\n",
    "        \n",
    "        # Create a separate validation dataset with test transforms\n",
    "        cifar_val = CIFAR100(self.data_dir, train=True, transform=self.transform_test)\n",
    "        _, val_indices = random_split(\n",
    "            range(len(cifar_full)), [train_size, val_size],\n",
    "            generator=torch.Generator().manual_seed(42)\n",
    "        )\n",
    "        self.val_dataset = torch.utils.data.Subset(cifar_val, val_indices.indices)\n",
    "        \n",
    "        self.test_dataset = CIFAR100(self.data_dir, train=False, transform=self.transform_test)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset, batch_size=self.batch_size, shuffle=True,\n",
    "            num_workers=self.num_workers, pin_memory=True\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset, batch_size=self.batch_size, shuffle=False,\n",
    "            num_workers=self.num_workers, pin_memory=True\n",
    "        )\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset, batch_size=self.batch_size, shuffle=False,\n",
    "            num_workers=self.num_workers, pin_memory=True\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "caef25c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdversarialTrainingModule(pl.LightningModule):\n",
    "    def __init__(self, \n",
    "                 pretrained_path=None,\n",
    "                 epsilon=8/255, \n",
    "                 alpha=2/255, \n",
    "                 attack_iters=7, \n",
    "                 trades_beta=8.0,\n",
    "                 learning_rate=0.05,\n",
    "                 momentum=0.9,\n",
    "                 weight_decay=5e-4,\n",
    "                 lr_scheduler=\"cosine\",\n",
    "                 max_epochs=200):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Model definition\n",
    "        self.model = models.resnet18(weights=None)\n",
    "        self.model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.model.fc = nn.Linear(self.model.fc.in_features, 100)\n",
    "        \n",
    "        # Load pretrained model if provided\n",
    "        if pretrained_path:\n",
    "            checkpoint = torch.load(pretrained_path)\n",
    "            self.model.load_state_dict(checkpoint['state_dict'])\n",
    "            print(f\"Loaded pretrained model from {pretrained_path}\")\n",
    "        \n",
    "        # Save clean and robust accuracy as attributes\n",
    "        self.best_clean_acc = 0.0\n",
    "        self.best_robust_acc = 0.0\n",
    "        \n",
    "        # Class weights for balanced loss\n",
    "        self.class_weights = None\n",
    "        self.weight_update_counter = 0\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def trades_loss(self, x_natural, y, perturb_steps=10):\n",
    "        \"\"\"TRADES loss for adversarial robustness\"\"\"\n",
    "        # Loss functions\n",
    "        criterion_kl = nn.KLDivLoss(reduction='batchmean')\n",
    "        \n",
    "        # Generate adversarial examples\n",
    "        x_adv = x_natural.detach() + 0.001 * torch.randn(x_natural.shape, device=self.device)\n",
    "        \n",
    "        for _ in range(perturb_steps):\n",
    "            x_adv.requires_grad_()\n",
    "            with torch.enable_grad():\n",
    "                loss_kl = criterion_kl(\n",
    "                    F.log_softmax(self.model(x_adv), dim=1),\n",
    "                    F.softmax(self.model(x_natural), dim=1)\n",
    "                )\n",
    "            grad = torch.autograd.grad(loss_kl, [x_adv])[0]\n",
    "            x_adv = x_adv.detach() + self.hparams.alpha * torch.sign(grad.detach())\n",
    "            x_adv = torch.min(torch.max(x_adv, x_natural - self.hparams.epsilon), \n",
    "                             x_natural + self.hparams.epsilon)\n",
    "            x_adv = torch.clamp(x_adv, 0.0, 1.0)\n",
    "        \n",
    "        # Calculate the TRADES loss\n",
    "        logits_natural = self.model(x_natural)\n",
    "        logits_adv = self.model(x_adv)\n",
    "        \n",
    "        if self.class_weights is not None and self.class_weights.device != self.device:\n",
    "            self.class_weights = self.class_weights.to(self.device)\n",
    "        \n",
    "        # Natural loss (possibly with class weights)\n",
    "        if self.class_weights is not None:\n",
    "            loss_natural = F.cross_entropy(logits_natural, y, weight=self.class_weights)\n",
    "        else:\n",
    "            loss_natural = F.cross_entropy(logits_natural, y)\n",
    "        \n",
    "        # KL divergence\n",
    "        loss_robust = criterion_kl(\n",
    "            F.log_softmax(logits_adv, dim=1),\n",
    "            F.softmax(logits_natural, dim=1)\n",
    "        )\n",
    "        \n",
    "        # Combined loss\n",
    "        loss = loss_natural + self.hparams.trades_beta * loss_robust\n",
    "        \n",
    "        return loss, logits_natural, logits_adv, x_adv\n",
    "    \n",
    "    def compute_class_weights(self, dataloader):\n",
    "        \"\"\"Compute class weights based on model confusion\"\"\"\n",
    "        confusion = torch.zeros(100, 100, device=self.device)\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in dataloader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(inputs)\n",
    "                _, preds = outputs.max(1)\n",
    "                for t, p in zip(labels.view(-1), preds.view(-1)):\n",
    "                    confusion[t.long(), p.long()] += 1\n",
    "        \n",
    "        # Normalize and compute weights\n",
    "        confusion = confusion / (confusion.sum(dim=1, keepdim=True) + 1e-8)\n",
    "        diag_indices = torch.arange(100, device=self.device)\n",
    "        confusion[diag_indices, diag_indices] = 0\n",
    "        class_weights = confusion.sum(dim=1)\n",
    "        \n",
    "        # Normalize weights\n",
    "        class_weights = class_weights / class_weights.mean()\n",
    "        class_weights = 0.5 + class_weights / 2\n",
    "        \n",
    "        return class_weights\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        \n",
    "        # Update class weights occasionally\n",
    "        if self.trainer.current_epoch > 0 and self.trainer.current_epoch % 15 == 0 and self.weight_update_counter != self.trainer.current_epoch:\n",
    "            self.weight_update_counter = self.trainer.current_epoch\n",
    "            self.class_weights = self.compute_class_weights(self.trainer.val_dataloaders[0])\n",
    "            self.log('class_weights_min', self.class_weights.min())\n",
    "            self.log('class_weights_max', self.class_weights.max())\n",
    "        \n",
    "        # Use TRADES loss\n",
    "        loss, logits_natural, logits_adv, adv_images = self.trades_loss(\n",
    "            inputs, labels, perturb_steps=self.hparams.attack_iters\n",
    "        )\n",
    "        \n",
    "        # Calculate metrics\n",
    "        _, nat_preds = logits_natural.max(1)\n",
    "        nat_acc = nat_preds.eq(labels).float().mean()\n",
    "        \n",
    "        _, adv_preds = logits_adv.max(1)\n",
    "        adv_acc = adv_preds.eq(labels).float().mean()\n",
    "        \n",
    "        # Log metrics\n",
    "        self.log('train_loss', loss, prog_bar=True, sync_dist=True)\n",
    "        self.log('train_nat_acc', nat_acc * 100.0, prog_bar=True, sync_dist=True)\n",
    "        self.log('train_adv_acc', adv_acc * 100.0, prog_bar=True, sync_dist=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        outputs = self.model(inputs)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        \n",
    "        _, preds = outputs.max(1)\n",
    "        acc = preds.eq(labels).float().mean()\n",
    "        \n",
    "        # Log metrics\n",
    "        self.log('val_loss', loss, prog_bar=True, sync_dist=True)\n",
    "        self.log('val_acc', acc * 100.0, prog_bar=True, sync_dist=True)\n",
    "        \n",
    "        return {'val_loss': loss, 'val_acc': acc}\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        \n",
    "        # Clean accuracy\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(inputs)\n",
    "            _, preds = outputs.max(1)\n",
    "            clean_acc = preds.eq(labels).float().mean()\n",
    "        \n",
    "        # Generate adversarial examples with PGD\n",
    "        x_adv = inputs.clone() + 0.001 * torch.randn(inputs.shape, device=self.device)\n",
    "        x_adv = torch.clamp(x_adv, 0.0, 1.0)\n",
    "        \n",
    "        for _ in range(20):  # More iterations for test-time evaluation\n",
    "            x_adv.requires_grad_()\n",
    "            with torch.enable_grad():\n",
    "                outputs_adv = self.model(x_adv)\n",
    "                loss = F.cross_entropy(outputs_adv, labels)\n",
    "            \n",
    "            grad = torch.autograd.grad(loss, [x_adv])[0]\n",
    "            x_adv = x_adv.detach() + self.hparams.alpha * torch.sign(grad.detach())\n",
    "            delta = torch.clamp(x_adv - inputs, -self.hparams.epsilon, self.hparams.epsilon)\n",
    "            x_adv = torch.clamp(inputs + delta, 0.0, 1.0)\n",
    "        \n",
    "        # Evaluate on adversarial examples\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(x_adv)\n",
    "            _, preds = outputs.max(1)\n",
    "            robust_acc = preds.eq(labels).float().mean()\n",
    "        \n",
    "        # Log metrics\n",
    "        self.log('test_clean_acc', clean_acc * 100.0, prog_bar=True, sync_dist=True)\n",
    "        self.log('test_robust_acc', robust_acc * 100.0, prog_bar=True, sync_dist=True)\n",
    "        \n",
    "        return {'test_clean_acc': clean_acc, 'test_robust_acc': robust_acc}\n",
    "    \n",
    "    def on_test_epoch_end(self):\n",
    "        # This is called automatically at the end of the test epoch\n",
    "        pass\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # Initialize optimizer with reduced learning rate for pretrained model\n",
    "        optimizer = optim.SGD(\n",
    "            self.parameters(),\n",
    "            lr=self.hparams.learning_rate,\n",
    "            momentum=self.hparams.momentum,\n",
    "            weight_decay=self.hparams.weight_decay\n",
    "        )\n",
    "        \n",
    "        # Configure learning rate scheduler\n",
    "        if self.hparams.lr_scheduler == \"cosine\":\n",
    "            scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "                optimizer, \n",
    "                T_max=self.hparams.max_epochs\n",
    "            )\n",
    "        else:\n",
    "            scheduler = optim.lr_scheduler.MultiStepLR(\n",
    "                optimizer,\n",
    "                milestones=[60, 120, 160],\n",
    "                gamma=0.2\n",
    "            )\n",
    "        \n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"monitor\": \"val_loss\"\n",
    "            }\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f578b28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_adversarial_model(\n",
    "    pretrained_path='best_resnet18_cifar100_untargeted_adv.pth',\n",
    "    batch_size=128,\n",
    "    max_epochs=200,\n",
    "    learning_rate=0.05,\n",
    "    epsilon=8/255,\n",
    "    alpha=2/255,\n",
    "    attack_iters=7,\n",
    "    trades_beta=8.0,\n",
    "    num_workers=4\n",
    "):\n",
    "    # Set up data module\n",
    "    data_module = CIFAR100DataModule(\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    \n",
    "    # Initialize model\n",
    "    model = AdversarialTrainingModule(\n",
    "        pretrained_path=pretrained_path,\n",
    "        epsilon=epsilon,\n",
    "        alpha=alpha,\n",
    "        attack_iters=attack_iters,\n",
    "        trades_beta=trades_beta,\n",
    "        learning_rate=learning_rate,\n",
    "        max_epochs=max_epochs\n",
    "    )\n",
    "    \n",
    "    # Define callbacks\n",
    "    checkpoint_callback_clean = ModelCheckpoint(\n",
    "        monitor='val_acc',\n",
    "        filename='best-clean-{epoch:02d}-{val_acc:.2f}',\n",
    "        save_top_k=1,\n",
    "        mode='max',\n",
    "        save_last=True\n",
    "    )\n",
    "    \n",
    "    checkpoint_callback_robust = ModelCheckpoint(\n",
    "        monitor='test_robust_acc',\n",
    "        filename='best-robust-{epoch:02d}-{test_robust_acc:.2f}',\n",
    "        save_top_k=1,\n",
    "        mode='max'\n",
    "    )\n",
    "    \n",
    "    lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "    \n",
    "    # Set up logger\n",
    "    logger = TensorBoardLogger(\"lightning_logs\", name=\"adversarial_training\")\n",
    "    \n",
    "    # Initialize trainer with GPU strategy\n",
    "    trainer = pl.Trainer(\n",
    "        accelerator='gpu',\n",
    "        devices=2,  # Using 2 GPUs\n",
    "        strategy='ddp_spawn',  # Distributed Data Parallel\n",
    "        max_epochs=max_epochs,\n",
    "        callbacks=[checkpoint_callback_clean, checkpoint_callback_robust, lr_monitor],\n",
    "        logger=logger,\n",
    "        precision=16,  # You can try 16 for mixed precision training\n",
    "        log_every_n_steps=50,\n",
    "        # Run testing every 3 epochs\n",
    "        check_val_every_n_epoch=3\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.fit(model, data_module)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    trainer.test(model, data_module)\n",
    "    \n",
    "    return model, trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "628d1367",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "/home/pnagaraj/miniconda3/lib/python3.12/site-packages/lightning_fabric/connector.py:571: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model from best_resnet18_cifar100_untargeted_adv.pth\n"
     ]
    },
    {
     "ename": "MisconfigurationException",
     "evalue": "`Trainer(strategy='ddp_spawn')` is not compatible with an interactive environment. Run your code as a script, or choose a notebook-compatible strategy: `Trainer(strategy='ddp_notebook')`. In case you are spawning processes yourself, make sure to include the Trainer creation inside the worker function.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMisconfigurationException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 10\u001b[0m\n\u001b[1;32m      3\u001b[0m pl\u001b[38;5;241m.\u001b[39mseed_everything(\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# # Make sure CUDA is available\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# print(f\"CUDA available: {torch.cuda.is_available()}\")\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# print(f\"Number of GPUs: {torch.cuda.device_count()}\")\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m model, trainer \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_adversarial_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbest_resnet18_cifar100_untargeted_adv.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Reduced from 0.1 since we're using a pretrained model\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattack_iters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrades_beta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8.0\u001b[39;49m\n\u001b[1;32m     19\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining completed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest clean accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39mbest_clean_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 51\u001b[0m, in \u001b[0;36mtrain_adversarial_model\u001b[0;34m(pretrained_path, batch_size, max_epochs, learning_rate, epsilon, alpha, attack_iters, trades_beta, num_workers)\u001b[0m\n\u001b[1;32m     48\u001b[0m logger \u001b[38;5;241m=\u001b[39m TensorBoardLogger(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlightning_logs\u001b[39m\u001b[38;5;124m\"\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madversarial_training\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Initialize trainer with GPU strategy\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Using 2 GPUs\u001b[39;49;00m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mddp_spawn\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Distributed Data Parallel\u001b[39;49;00m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint_callback_clean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_callback_robust\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_monitor\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprecision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# You can try 16 for mixed precision training\u001b[39;49;00m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_every_n_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Run testing every 3 epochs\u001b[39;49;00m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck_val_every_n_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\n\u001b[1;32m     62\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m     65\u001b[0m trainer\u001b[38;5;241m.\u001b[39mfit(model, data_module)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pytorch_lightning/utilities/argparse.py:70\u001b[0m, in \u001b[0;36m_defaults_from_env_vars.<locals>.insert_env_defaults\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mlist\u001b[39m(env_variables\u001b[38;5;241m.\u001b[39mitems()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(kwargs\u001b[38;5;241m.\u001b[39mitems()))\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# all args were already moved to kwargs\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:404\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, accelerator, strategy, devices, num_nodes, precision, logger, callbacks, fast_dev_run, max_epochs, min_epochs, max_steps, min_steps, max_time, limit_train_batches, limit_val_batches, limit_test_batches, limit_predict_batches, overfit_batches, val_check_interval, check_val_every_n_epoch, num_sanity_val_steps, log_every_n_steps, enable_checkpointing, enable_progress_bar, enable_model_summary, accumulate_grad_batches, gradient_clip_val, gradient_clip_algorithm, deterministic, benchmark, inference_mode, use_distributed_sampler, profiler, detect_anomaly, barebones, plugins, sync_batchnorm, reload_dataloaders_every_n_epochs, default_root_dir, model_registry)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;66;03m# init connectors\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_connector \u001b[38;5;241m=\u001b[39m _DataConnector(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 404\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerator_connector \u001b[38;5;241m=\u001b[39m \u001b[43m_AcceleratorConnector\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_nodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[43msync_batchnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msync_batchnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbenchmark\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbenchmark\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_distributed_sampler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_distributed_sampler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprecision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplugins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplugins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger_connector \u001b[38;5;241m=\u001b[39m _LoggerConnector(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_connector \u001b[38;5;241m=\u001b[39m _CallbackConnector(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:163\u001b[0m, in \u001b[0;36m_AcceleratorConnector.__init__\u001b[0;34m(self, devices, num_nodes, accelerator, strategy, plugins, precision, sync_batchnorm, benchmark, use_distributed_sampler, deterministic)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_and_init_precision()\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# 6. Instantiate Strategy - Part 2\u001b[39;00m\n\u001b[0;32m--> 163\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lazy_init_strategy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:581\u001b[0m, in \u001b[0;36m_AcceleratorConnector._lazy_init_strategy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m_configure_launcher()\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _IS_INTERACTIVE \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mis_interactive_compatible:\n\u001b[0;32m--> 581\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`Trainer(strategy=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy_flag\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m)` is not compatible with an interactive\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    583\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m environment. Run your code as a script, or choose a notebook-compatible strategy:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    584\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `Trainer(strategy=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mddp_notebook\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    585\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m In case you are spawning processes yourself, make sure to include the Trainer\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    586\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m creation inside the worker function.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    587\u001b[0m     )\n\u001b[1;32m    589\u001b[0m \u001b[38;5;66;03m# TODO: should be moved to _check_strategy_and_fallback().\u001b[39;00m\n\u001b[1;32m    590\u001b[0m \u001b[38;5;66;03m# Current test check precision first, so keep this check here to meet error order\u001b[39;00m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator, XLAAccelerator) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    592\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy, (SingleDeviceXLAStrategy, XLAStrategy)\n\u001b[1;32m    593\u001b[0m ):\n",
      "\u001b[0;31mMisconfigurationException\u001b[0m: `Trainer(strategy='ddp_spawn')` is not compatible with an interactive environment. Run your code as a script, or choose a notebook-compatible strategy: `Trainer(strategy='ddp_notebook')`. In case you are spawning processes yourself, make sure to include the Trainer creation inside the worker function."
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Set random seeds for reproducibility\n",
    "    pl.seed_everything(42)\n",
    "    \n",
    "    # # Make sure CUDA is available\n",
    "    # print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    # print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    \n",
    "    # Start training\n",
    "    model, trainer = train_adversarial_model(\n",
    "        pretrained_path='best_resnet18_cifar100_untargeted_adv.pth',\n",
    "        batch_size=128,\n",
    "        max_epochs=200,\n",
    "        learning_rate=0.05,  # Reduced from 0.1 since we're using a pretrained model\n",
    "        epsilon=8/255,\n",
    "        alpha=2/255,\n",
    "        attack_iters=7,\n",
    "        trades_beta=8.0\n",
    "    )\n",
    "    \n",
    "    print(\"Training completed!\")\n",
    "    print(f\"Best clean accuracy: {model.best_clean_acc:.2f}%\")\n",
    "    print(f\"Best robust accuracy: {model.best_robust_acc:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

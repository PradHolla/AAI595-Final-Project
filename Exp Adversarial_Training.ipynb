{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import random_split\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torchvision.datasets import CIFAR100\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations for CIFAR-100\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.AutoAugment(transforms.AutoAugmentPolicy.CIFAR10),  # Stronger augmentation\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
    "])\n",
    "\n",
    "# Load the full training dataset with training transformations\n",
    "full_train_dataset = CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
    "\n",
    "# Create a validation split from the training data\n",
    "val_size = int(0.1 * len(full_train_dataset))  # 10% for validation\n",
    "train_size = len(full_train_dataset) - val_size\n",
    "\n",
    "# Use random_split to create the training and validation datasets\n",
    "train_dataset, val_dataset = random_split(\n",
    "    full_train_dataset, \n",
    "    [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(42)  # For reproducibility\n",
    ")\n",
    "\n",
    "# For the validation set, we need to replace the transform\n",
    "# Create a copy of the validation dataset with test transforms\n",
    "val_dataset = CIFAR100(root='./data', train=True, download=False, transform=transform_test)\n",
    "\n",
    "# Only use the validation indices\n",
    "val_indices = val_dataset.indices if hasattr(val_dataset, 'indices') else range(len(full_train_dataset))[train_size:]\n",
    "val_dataset = torch.utils.data.Subset(val_dataset, val_indices)\n",
    "\n",
    "test_dataset = CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pnagaraj/miniconda3/lib/python3.12/site-packages/torch/cuda/__init__.py:734: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "# Initialize ResNet18 model for CIFAR100 (100 classes)\n",
    "model = models.resnet18(weights=None)  # Start from scratch for a true baseline\n",
    "\n",
    "model.conv1 = torch.nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "\n",
    "# Adjust final fully connected layer for 100 classes\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, 100)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated improved_pgd_attack function\n",
    "def improved_pgd_attack(model, images, labels, target_labels=None, epsilon=4/255, alpha=1/255, \n",
    "                       iters=3, random_start=True, untargeted=False):\n",
    "    \"\"\"\n",
    "    Enhanced PGD attack with better performance and stability.\n",
    "    Works with any input tensor shape.\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    images = images.clone().detach().to(device)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    if target_labels is not None:\n",
    "        target_labels = target_labels.to(device)\n",
    "    \n",
    "    # Loss function with per-sample losses\n",
    "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "    \n",
    "    # Initialize adversarial images\n",
    "    adv_images = images.clone().detach()\n",
    "    \n",
    "    # Start with random noise within epsilon ball if requested\n",
    "    if random_start:\n",
    "        noise = torch.FloatTensor(images.shape).uniform_(-epsilon, epsilon).to(device)\n",
    "        adv_images = adv_images + noise\n",
    "        adv_images = torch.clamp(adv_images, 0, 1)\n",
    "    \n",
    "    best_adv_images = adv_images.clone()\n",
    "    best_loss = None  # Will be a tensor of shape (batch_size,)\n",
    "    \n",
    "    for i in range(iters):\n",
    "        adv_images.requires_grad = True\n",
    "        \n",
    "        outputs = model(adv_images)\n",
    "        \n",
    "        # Compute per-sample loss\n",
    "        if untargeted:\n",
    "            # For untargeted, maximize loss on true labels\n",
    "            loss_per_sample = criterion(outputs, labels)  # Shape: (batch_size,)\n",
    "            loss = -loss_per_sample.mean()  # Scalar for gradient computation\n",
    "        else:\n",
    "            if target_labels is None:\n",
    "                raise ValueError(\"Target labels must be provided for targeted attack\")\n",
    "            # For targeted, minimize loss on target labels\n",
    "            loss_per_sample = criterion(outputs, target_labels)\n",
    "            loss = loss_per_sample.mean()\n",
    "        \n",
    "        # Check for NaN or inf in loss\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            print(f\"Warning: Invalid loss detected at iteration {i}. Returning last valid images.\")\n",
    "            return best_adv_images.detach()\n",
    "        \n",
    "        # Clear gradients to prevent accumulation\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient step\n",
    "        with torch.no_grad():\n",
    "            grad = adv_images.grad.sign()\n",
    "            if untargeted:\n",
    "                adv_images = adv_images + alpha * grad  # Increase loss\n",
    "            else:\n",
    "                adv_images = adv_images - alpha * grad  # Decrease loss\n",
    "            delta = torch.clamp(adv_images - images, min=-epsilon, max=epsilon)\n",
    "            adv_images = torch.clamp(images + delta, min=0, max=1)\n",
    "            \n",
    "            # Check for NaN or inf in adv_images\n",
    "            if torch.isnan(adv_images).any() or torch.isinf(adv_images).any():\n",
    "                print(f\"Warning: Invalid adv_images detected at iteration {i}. Returning last valid images.\")\n",
    "                return best_adv_images.detach()\n",
    "        \n",
    "        # Track best adversarial images\n",
    "        with torch.no_grad():\n",
    "            _, predicted = outputs.max(1)\n",
    "            if untargeted:\n",
    "                success_mask = (predicted != labels)\n",
    "                # Maximize loss_per_sample\n",
    "                if best_loss is None:\n",
    "                    best_loss = loss_per_sample.clone()\n",
    "                    best_adv_images = adv_images.clone()\n",
    "                else:\n",
    "                    improved = loss_per_sample > best_loss  # Want larger loss\n",
    "                    update_indices = torch.nonzero(improved & success_mask).view(-1)\n",
    "                    if len(update_indices) > 0:\n",
    "                        best_adv_images[update_indices] = adv_images[update_indices]\n",
    "                        best_loss[update_indices] = loss_per_sample[update_indices]\n",
    "            else:\n",
    "                success_mask = (predicted == target_labels)\n",
    "                # Minimize loss_per_sample\n",
    "                if best_loss is None:\n",
    "                    best_loss = loss_per_sample.clone()\n",
    "                    best_adv_images = adv_images.clone()\n",
    "                else:\n",
    "                    improved = loss_per_sample < best_loss  # Want smaller loss\n",
    "                    update_indices = torch.nonzero(improved & success_mask).view(-1)\n",
    "                    if len(update_indices) > 0:\n",
    "                        best_adv_images[update_indices] = adv_images[update_indices]\n",
    "                        best_loss[update_indices] = loss_per_sample[update_indices]\n",
    "    \n",
    "    return best_adv_images.detach()\n",
    "\n",
    "def improved_evaluate_attack(model, testloader, epsilon=20/255, alpha=8/255, iters=100, subset_size=None):\n",
    "    \"\"\"\n",
    "    Evaluates the model on clean and adversarial examples with improved attack\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    clean_correct = 0\n",
    "    adv_correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Handle subset processing\n",
    "    if subset_size:\n",
    "        try:\n",
    "            subset_loader = torch.utils.data.DataLoader(\n",
    "                testset, batch_size=128, shuffle=True, num_workers=4\n",
    "            )\n",
    "        except NameError:\n",
    "            subset_loader = testloader\n",
    "        max_batches = subset_size // 128 + 1\n",
    "    else:\n",
    "        subset_loader = testloader\n",
    "        max_batches = len(testloader)\n",
    "    \n",
    "    for i, (images, labels) in enumerate(subset_loader):\n",
    "        if i >= max_batches:\n",
    "            break\n",
    "            \n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        # Clean accuracy\n",
    "        with torch.no_grad():\n",
    "            outputs = model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            clean_correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        # Generate adversarial examples using untargeted attack (more effective)\n",
    "        adv_images = improved_pgd_attack(\n",
    "            model, \n",
    "            images, \n",
    "            labels,\n",
    "            target_labels=None, \n",
    "            epsilon=epsilon, \n",
    "            alpha=alpha, \n",
    "            iters=iters,\n",
    "            random_start=True,\n",
    "            untargeted=True  # Untargeted attacks are typically more successful\n",
    "        )\n",
    "        \n",
    "        # Evaluate adversarial accuracy\n",
    "        with torch.no_grad():\n",
    "            adv_outputs = model(adv_images)\n",
    "            _, adv_predicted = adv_outputs.max(1)\n",
    "            adv_success = (adv_predicted != labels).sum().item()  # Count misclassifications\n",
    "            adv_correct += adv_success\n",
    "    \n",
    "    clean_acc = 100. * clean_correct / total\n",
    "    attack_success = 100. * adv_correct / total\n",
    "    print(f\"Clean Accuracy: {clean_acc:.2f}%\")\n",
    "    print(f\"Attack Success Rate: {attack_success:.2f}%\")\n",
    "    return clean_acc, attack_success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# SGD with momentum typically works better for adversarial training\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "# Better learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrain on Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 [Train]: 100%|██████████| 391/391 [00:17<00:00, 22.89it/s, loss=4.2746, acc=5.40%, lr=0.100000]\n",
      "Epoch 1/20 [Test]: 100%|██████████| 79/79 [00:01<00:00, 52.07it/s, loss=3.8234, acc=11.09%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved! New best accuracy: 11.09%\n",
      "Epoch: 1/20 | Train Loss: 4.2746 | Train Acc: 5.40% | Test Loss: 3.8234 | Test Acc: 11.09%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 [Train]: 100%|██████████| 391/391 [00:16<00:00, 23.86it/s, loss=3.7627, acc=12.01%, lr=0.099994]\n",
      "Epoch 2/20 [Test]: 100%|██████████| 79/79 [00:01<00:00, 52.39it/s, loss=3.2825, acc=19.82%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved! New best accuracy: 19.82%\n",
      "Epoch: 2/20 | Train Loss: 3.7627 | Train Acc: 12.01% | Test Loss: 3.2825 | Test Acc: 19.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 [Train]: 100%|██████████| 391/391 [00:16<00:00, 23.90it/s, loss=3.4113, acc=18.00%, lr=0.099975]\n",
      "Epoch 3/20 [Test]: 100%|██████████| 79/79 [00:01<00:00, 53.36it/s, loss=2.8535, acc=27.55%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved! New best accuracy: 27.55%\n",
      "Epoch: 3/20 | Train Loss: 3.4113 | Train Acc: 18.00% | Test Loss: 2.8535 | Test Acc: 27.55%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 [Train]: 100%|██████████| 391/391 [00:16<00:00, 23.92it/s, loss=3.1465, acc=22.94%, lr=0.099944]\n",
      "Epoch 4/20 [Test]: 100%|██████████| 79/79 [00:01<00:00, 52.83it/s, loss=2.7944, acc=29.00%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved! New best accuracy: 29.00%\n",
      "Epoch: 4/20 | Train Loss: 3.1465 | Train Acc: 22.94% | Test Loss: 2.7944 | Test Acc: 29.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 [Train]: 100%|██████████| 391/391 [00:16<00:00, 23.74it/s, loss=2.9714, acc=25.98%, lr=0.099901]\n",
      "Epoch 5/20 [Test]: 100%|██████████| 79/79 [00:01<00:00, 53.38it/s, loss=2.7871, acc=30.39%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved! New best accuracy: 30.39%\n",
      "Epoch: 5/20 | Train Loss: 2.9714 | Train Acc: 25.98% | Test Loss: 2.7871 | Test Acc: 30.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 [Train]: 100%|██████████| 391/391 [00:16<00:00, 23.83it/s, loss=2.8145, acc=29.07%, lr=0.099846]\n",
      "Epoch 6/20 [Test]: 100%|██████████| 79/79 [00:01<00:00, 52.76it/s, loss=2.5405, acc=34.18%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved! New best accuracy: 34.18%\n",
      "Epoch: 6/20 | Train Loss: 2.8145 | Train Acc: 29.07% | Test Loss: 2.5405 | Test Acc: 34.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 [Train]: 100%|██████████| 391/391 [00:16<00:00, 23.97it/s, loss=2.7182, acc=31.32%, lr=0.099778]\n",
      "Epoch 7/20 [Test]: 100%|██████████| 79/79 [00:01<00:00, 53.13it/s, loss=2.6800, acc=32.72%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/20 | Train Loss: 2.7182 | Train Acc: 31.32% | Test Loss: 2.6800 | Test Acc: 32.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 [Train]: 100%|██████████| 391/391 [00:16<00:00, 23.84it/s, loss=2.6112, acc=33.43%, lr=0.099698]\n",
      "Epoch 8/20 [Test]: 100%|██████████| 79/79 [00:01<00:00, 53.28it/s, loss=2.4166, acc=37.68%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved! New best accuracy: 37.68%\n",
      "Epoch: 8/20 | Train Loss: 2.6112 | Train Acc: 33.43% | Test Loss: 2.4166 | Test Acc: 37.68%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 [Train]: 100%|██████████| 391/391 [00:16<00:00, 23.86it/s, loss=2.5389, acc=35.10%, lr=0.099606]\n",
      "Epoch 9/20 [Test]: 100%|██████████| 79/79 [00:01<00:00, 52.62it/s, loss=2.2593, acc=40.78%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved! New best accuracy: 40.78%\n",
      "Epoch: 9/20 | Train Loss: 2.5389 | Train Acc: 35.10% | Test Loss: 2.2593 | Test Acc: 40.78%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 [Train]: 100%|██████████| 391/391 [00:16<00:00, 23.91it/s, loss=2.4755, acc=36.50%, lr=0.099501]\n",
      "Epoch 10/20 [Test]: 100%|██████████| 79/79 [00:01<00:00, 52.84it/s, loss=2.4373, acc=37.53%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/20 | Train Loss: 2.4755 | Train Acc: 36.50% | Test Loss: 2.4373 | Test Acc: 37.53%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 [Train]: 100%|██████████| 391/391 [00:16<00:00, 23.93it/s, loss=2.4263, acc=37.15%, lr=0.099384]\n",
      "Epoch 11/20 [Test]: 100%|██████████| 79/79 [00:01<00:00, 53.15it/s, loss=2.1466, acc=42.94%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved! New best accuracy: 42.94%\n",
      "Epoch: 11/20 | Train Loss: 2.4263 | Train Acc: 37.15% | Test Loss: 2.1466 | Test Acc: 42.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 [Train]: 100%|██████████| 391/391 [00:16<00:00, 23.87it/s, loss=2.3680, acc=38.48%, lr=0.099255]\n",
      "Epoch 12/20 [Test]: 100%|██████████| 79/79 [00:01<00:00, 53.46it/s, loss=2.0424, acc=45.10%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved! New best accuracy: 45.10%\n",
      "Epoch: 12/20 | Train Loss: 2.3680 | Train Acc: 38.48% | Test Loss: 2.0424 | Test Acc: 45.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 [Train]: 100%|██████████| 391/391 [00:16<00:00, 23.98it/s, loss=2.3270, acc=39.81%, lr=0.099114]\n",
      "Epoch 13/20 [Test]: 100%|██████████| 79/79 [00:01<00:00, 53.23it/s, loss=2.4313, acc=38.83%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13/20 | Train Loss: 2.3270 | Train Acc: 39.81% | Test Loss: 2.4313 | Test Acc: 38.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 [Train]: 100%|██████████| 391/391 [00:16<00:00, 23.96it/s, loss=2.2965, acc=40.43%, lr=0.098961]\n",
      "Epoch 14/20 [Test]: 100%|██████████| 79/79 [00:01<00:00, 51.95it/s, loss=2.2776, acc=41.22%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/20 | Train Loss: 2.2965 | Train Acc: 40.43% | Test Loss: 2.2776 | Test Acc: 41.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 [Train]: 100%|██████████| 391/391 [00:16<00:00, 23.76it/s, loss=2.2624, acc=40.92%, lr=0.098796]\n",
      "Epoch 15/20 [Test]: 100%|██████████| 79/79 [00:01<00:00, 52.43it/s, loss=2.0078, acc=46.44%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved! New best accuracy: 46.44%\n",
      "Epoch: 15/20 | Train Loss: 2.2624 | Train Acc: 40.92% | Test Loss: 2.0078 | Test Acc: 46.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 [Train]: 100%|██████████| 391/391 [00:16<00:00, 23.78it/s, loss=2.2243, acc=42.02%, lr=0.098618]\n",
      "Epoch 16/20 [Test]: 100%|██████████| 79/79 [00:01<00:00, 53.34it/s, loss=2.3144, acc=40.54%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16/20 | Train Loss: 2.2243 | Train Acc: 42.02% | Test Loss: 2.3144 | Test Acc: 40.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 [Train]: 100%|██████████| 391/391 [00:16<00:00, 23.88it/s, loss=2.2191, acc=41.83%, lr=0.098429]\n",
      "Epoch 17/20 [Test]: 100%|██████████| 79/79 [00:01<00:00, 52.55it/s, loss=1.9776, acc=47.22%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved! New best accuracy: 47.22%\n",
      "Epoch: 17/20 | Train Loss: 2.2191 | Train Acc: 41.83% | Test Loss: 1.9776 | Test Acc: 47.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 [Train]: 100%|██████████| 391/391 [00:16<00:00, 24.16it/s, loss=2.1772, acc=42.95%, lr=0.098228]\n",
      "Epoch 18/20 [Test]: 100%|██████████| 79/79 [00:01<00:00, 52.31it/s, loss=2.1846, acc=43.80%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18/20 | Train Loss: 2.1772 | Train Acc: 42.95% | Test Loss: 2.1846 | Test Acc: 43.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 [Train]: 100%|██████████| 391/391 [00:16<00:00, 23.86it/s, loss=2.1677, acc=43.24%, lr=0.098015]\n",
      "Epoch 19/20 [Test]: 100%|██████████| 79/79 [00:01<00:00, 51.83it/s, loss=1.8595, acc=49.26%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved! New best accuracy: 49.26%\n",
      "Epoch: 19/20 | Train Loss: 2.1677 | Train Acc: 43.24% | Test Loss: 1.8595 | Test Acc: 49.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 [Train]: 100%|██████████| 391/391 [00:16<00:00, 23.94it/s, loss=2.1471, acc=43.51%, lr=0.097790]\n",
      "Epoch 20/20 [Test]: 100%|██████████| 79/79 [00:01<00:00, 53.22it/s, loss=1.8369, acc=50.77%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved! New best accuracy: 50.77%\n",
      "Epoch: 20/20 | Train Loss: 2.1471 | Train Acc: 43.51% | Test Loss: 1.8369 | Test Acc: 50.77%\n",
      "Training completed! Best accuracy: 50.77%\n"
     ]
    }
   ],
   "source": [
    "# Training loop with tqdm\n",
    "num_epochs = 20\n",
    "best_acc = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    train_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]')\n",
    "    for inputs, labels in train_bar:\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        # Forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        train_bar.set_postfix({\n",
    "            'loss': f'{train_loss/len(train_bar):.4f}',\n",
    "            'acc': f'{100.*correct/total:.2f}%',\n",
    "            'lr': f'{scheduler.get_last_lr()[0]:.6f}'\n",
    "        })\n",
    "    \n",
    "    train_loss = train_loss / len(train_loader)\n",
    "    train_acc = 100. * correct / total\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    test_bar = tqdm(test_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Test]')\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_bar:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            test_bar.set_postfix({\n",
    "                'loss': f'{test_loss/len(test_bar):.4f}',\n",
    "                'acc': f'{100.*correct/total:.2f}%'\n",
    "            })\n",
    "    \n",
    "    test_loss = test_loss / len(test_loader)\n",
    "    test_acc = 100. * correct / total\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Save checkpoint if it's the best model so far\n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'scheduler': scheduler.state_dict(),\n",
    "            'best_acc': best_acc,\n",
    "        }\n",
    "        torch.save(checkpoint, 'best_resnet18_cifar100_untargeted_adv.pth')\n",
    "        print(f'Checkpoint saved! New best accuracy: {best_acc:.2f}%')\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f'Epoch: {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.2f}%')\n",
    "\n",
    "print(f\"Training completed! Best accuracy: {best_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on Adversarial Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRADES loss function\n",
    "def trades_loss(model, x_natural, y, optimizer, step_size=0.003, epsilon=0.031, perturb_steps=10, beta=6.0):\n",
    "    criterion_kl = nn.KLDivLoss(reduction='batchmean')\n",
    "    model.eval()  # Use evaluation mode for generating adversaries\n",
    "    batch_size = len(x_natural)\n",
    "    \n",
    "    # Generate adversarial examples\n",
    "    x_adv = x_natural.detach() + 0.001 * torch.randn(x_natural.shape).to(device).detach()\n",
    "    \n",
    "    for _ in range(perturb_steps):\n",
    "        x_adv.requires_grad_()\n",
    "        with torch.enable_grad():\n",
    "            loss_kl = criterion_kl(\n",
    "                F.log_softmax(model(x_adv), dim=1),\n",
    "                F.softmax(model(x_natural), dim=1)\n",
    "            )\n",
    "        grad = torch.autograd.grad(loss_kl, [x_adv])[0]\n",
    "        x_adv = x_adv.detach() + step_size * torch.sign(grad.detach())\n",
    "        x_adv = torch.min(torch.max(x_adv, x_natural - epsilon), x_natural + epsilon)\n",
    "        x_adv = torch.clamp(x_adv, 0.0, 1.0)\n",
    "    \n",
    "    model.train()  # Switch back to training mode\n",
    "    \n",
    "    # Calculate the TRADES loss\n",
    "    logits_natural = model(x_natural)\n",
    "    logits_adv = model(x_adv)\n",
    "    loss_natural = F.cross_entropy(logits_natural, y)\n",
    "    loss_robust = criterion_kl(\n",
    "        F.log_softmax(logits_adv, dim=1),\n",
    "        F.softmax(logits_natural, dim=1)\n",
    "    )\n",
    "    \n",
    "    # Total loss\n",
    "    loss = loss_natural + beta * loss_robust\n",
    "    return loss, logits_natural, logits_adv, x_adv\n",
    "\n",
    "# Function to compute class weights based on model confusion\n",
    "def compute_class_weights(model, dataloader, num_classes=100):\n",
    "    confusion = torch.zeros(num_classes, num_classes).to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc=\"Computing class weights\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = outputs.max(1)\n",
    "            for t, p in zip(labels.view(-1), preds.view(-1)):\n",
    "                confusion[t.long(), p.long()] += 1\n",
    "    \n",
    "    # Normalize by row sums\n",
    "    confusion = confusion / (confusion.sum(dim=1, keepdim=True) + 1e-8)\n",
    "    \n",
    "    # Class weights: higher weight for more confused classes\n",
    "    # We use the sum of off-diagonal elements as confusion score\n",
    "    diag_indices = torch.arange(num_classes)\n",
    "    confusion[diag_indices, diag_indices] = 0\n",
    "    class_weights = confusion.sum(dim=1)\n",
    "    \n",
    "    # Normalize weights\n",
    "    class_weights = class_weights / class_weights.mean()\n",
    "    \n",
    "    # Scale weights to be in a reasonable range\n",
    "    class_weights = 0.5 + class_weights / 2\n",
    "    \n",
    "    return class_weights\n",
    "\n",
    "# Evaluate adversarial robustness using PGD attack\n",
    "def evaluate_robustness(model, dataloader, epsilon=8/255, alpha=2/255, iters=20):\n",
    "    model.eval()\n",
    "    robust_correct = 0\n",
    "    clean_correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, targets in tqdm(dataloader, desc=\"Evaluating robustness\"):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Clean accuracy\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            clean_correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        # Create adversarial examples with PGD\n",
    "        x_adv = inputs.clone().detach() + 0.001 * torch.randn(inputs.shape).to(device).detach()\n",
    "        x_adv = torch.clamp(x_adv, 0.0, 1.0)\n",
    "        \n",
    "        for _ in range(iters):\n",
    "            x_adv.requires_grad_()\n",
    "            with torch.enable_grad():\n",
    "                outputs_adv = model(x_adv)\n",
    "                loss = F.cross_entropy(outputs_adv, targets)\n",
    "            \n",
    "            grad = torch.autograd.grad(loss, [x_adv])[0]\n",
    "            x_adv = x_adv.detach() + alpha * torch.sign(grad.detach())\n",
    "            delta = torch.clamp(x_adv - inputs, -epsilon, epsilon)\n",
    "            x_adv = torch.clamp(inputs + delta, 0.0, 1.0)\n",
    "        \n",
    "        # Evaluate on adversarial examples\n",
    "        with torch.no_grad():\n",
    "            outputs = model(x_adv)\n",
    "            _, predicted = outputs.max(1)\n",
    "            robust_correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        total += targets.size(0)\n",
    "    \n",
    "    clean_acc = 100.0 * clean_correct / total\n",
    "    robust_acc = 100.0 * robust_correct / total\n",
    "    \n",
    "    return clean_acc, robust_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model with clean accuracy around 50%\n",
      "Computing initial class weights...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing class weights: 100%|██████████| 40/40 [00:12<00:00,  3.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights range: Min=0.5553, Max=1.5848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 [Train]: 100%|██████████| 352/352 [02:00<00:00,  2.93it/s, loss=16.9556, nat_acc=45.35%, adv_acc=13.89%, lr=0.097553]\n",
      "Epoch 1/200 [Val]: 100%|██████████| 40/40 [00:00<00:00, 47.49it/s, loss=1.9793, acc=47.18%]\n",
      "Evaluating robustness: 100%|██████████| 79/79 [00:42<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Test Acc: 44.56% | Robust Test Acc: 10.72%\n",
      "Robust checkpoint saved! New best robust accuracy: 10.72%\n",
      "Clean checkpoint saved! New best accuracy: 47.18%\n",
      "Epoch: 1/200 | Train Loss: 16.9556 | Train Acc: 45.35% | Val Loss: 1.9793 | Val Acc: 47.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/200 [Train]: 100%|██████████| 352/352 [01:59<00:00,  2.96it/s, loss=16.9971, nat_acc=45.12%, adv_acc=22.22%, lr=0.048652]\n",
      "Epoch 2/200 [Val]: 100%|██████████| 40/40 [00:00<00:00, 49.38it/s, loss=1.9632, acc=47.58%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean checkpoint saved! New best accuracy: 47.58%\n",
      "Epoch: 2/200 | Train Loss: 16.9971 | Train Acc: 45.12% | Val Loss: 1.9632 | Val Acc: 47.58%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/200 [Train]: 100%|██████████| 352/352 [01:59<00:00,  2.95it/s, loss=17.0234, nat_acc=44.96%, adv_acc=12.50%, lr=0.048522]\n",
      "Epoch 3/200 [Val]: 100%|██████████| 40/40 [00:00<00:00, 46.78it/s, loss=1.9777, acc=46.90%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/200 | Train Loss: 17.0234 | Train Acc: 44.96% | Val Loss: 1.9777 | Val Acc: 46.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/200 [Train]: 100%|██████████| 352/352 [01:59<00:00,  2.96it/s, loss=17.0861, nat_acc=45.06%, adv_acc=16.67%, lr=0.048386]\n",
      "Epoch 4/200 [Val]: 100%|██████████| 40/40 [00:00<00:00, 47.44it/s, loss=1.9615, acc=47.62%]\n",
      "Evaluating robustness: 100%|██████████| 79/79 [00:42<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Test Acc: 44.68% | Robust Test Acc: 10.64%\n",
      "Clean checkpoint saved! New best accuracy: 47.62%\n",
      "Epoch: 4/200 | Train Loss: 17.0861 | Train Acc: 45.06% | Val Loss: 1.9615 | Val Acc: 47.62%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/200 [Train]: 100%|██████████| 352/352 [01:59<00:00,  2.95it/s, loss=17.0022, nat_acc=45.42%, adv_acc=16.67%, lr=0.048244]\n",
      "Epoch 5/200 [Val]: 100%|██████████| 40/40 [00:00<00:00, 47.24it/s, loss=1.9822, acc=46.94%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/200 | Train Loss: 17.0022 | Train Acc: 45.42% | Val Loss: 1.9822 | Val Acc: 46.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/200 [Train]: 100%|██████████| 352/352 [01:58<00:00,  2.96it/s, loss=16.9560, nat_acc=45.47%, adv_acc=23.61%, lr=0.048097]\n",
      "Epoch 6/200 [Val]: 100%|██████████| 40/40 [00:00<00:00, 45.06it/s, loss=1.9770, acc=47.08%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/200 | Train Loss: 16.9560 | Train Acc: 45.47% | Val Loss: 1.9770 | Val Acc: 47.08%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/200 [Train]: 100%|██████████| 352/352 [01:59<00:00,  2.95it/s, loss=17.0599, nat_acc=45.51%, adv_acc=19.44%, lr=0.047944]\n",
      "Epoch 7/200 [Val]: 100%|██████████| 40/40 [00:00<00:00, 46.74it/s, loss=1.9568, acc=47.62%]\n",
      "Evaluating robustness: 100%|██████████| 79/79 [00:42<00:00,  1.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Test Acc: 44.58% | Robust Test Acc: 10.59%\n",
      "Epoch: 7/200 | Train Loss: 17.0599 | Train Acc: 45.51% | Val Loss: 1.9568 | Val Acc: 47.62%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/200 [Train]: 100%|██████████| 352/352 [01:59<00:00,  2.96it/s, loss=17.1161, nat_acc=45.33%, adv_acc=18.06%, lr=0.047785]\n",
      "Epoch 8/200 [Val]: 100%|██████████| 40/40 [00:00<00:00, 47.60it/s, loss=1.9660, acc=47.36%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/200 | Train Loss: 17.1161 | Train Acc: 45.33% | Val Loss: 1.9660 | Val Acc: 47.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/200 [Train]:  87%|████████▋ | 307/352 [01:43<00:15,  2.96it/s, loss=16.9784, nat_acc=45.42%, adv_acc=17.19%, lr=0.047621]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Use TRADES loss\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m loss, logits_natural, logits_adv, adv_images \u001b[38;5;241m=\u001b[39m \u001b[43mtrades_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstep_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mperturb_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8.0\u001b[39;49m\n\u001b[1;32m     53\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Check for NaN or inf in loss\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misnan(loss) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(loss):\n",
      "Cell \u001b[0;32mIn[18], line 17\u001b[0m, in \u001b[0;36mtrades_loss\u001b[0;34m(model, x_natural, y, optimizer, step_size, epsilon, perturb_steps, beta)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[1;32m     13\u001b[0m     loss_kl \u001b[38;5;241m=\u001b[39m criterion_kl(\n\u001b[1;32m     14\u001b[0m         F\u001b[38;5;241m.\u001b[39mlog_softmax(model(x_adv), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     15\u001b[0m         F\u001b[38;5;241m.\u001b[39msoftmax(model(x_natural), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     16\u001b[0m     )\n\u001b[0;32m---> 17\u001b[0m grad \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_kl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mx_adv\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     18\u001b[0m x_adv \u001b[38;5;241m=\u001b[39m x_adv\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m+\u001b[39m step_size \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39msign(grad\u001b[38;5;241m.\u001b[39mdetach())\n\u001b[1;32m     19\u001b[0m x_adv \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmin(torch\u001b[38;5;241m.\u001b[39mmax(x_adv, x_natural \u001b[38;5;241m-\u001b[39m epsilon), x_natural \u001b[38;5;241m+\u001b[39m epsilon)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/autograd/__init__.py:496\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    492\u001b[0m     result \u001b[38;5;241m=\u001b[39m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[1;32m    493\u001b[0m         grad_outputs_\n\u001b[1;32m    494\u001b[0m     )\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    507\u001b[0m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[1;32m    508\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[1;32m    509\u001b[0m     ):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Pretrained model\n",
    "model = models.resnet18(weights=None)\n",
    "model.conv1 = torch.nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "model.fc = nn.Linear(model.fc.in_features, 100)  # 100 classes for CIFAR-100\n",
    "\n",
    "pretrained_checkpoint = torch.load('best_resnet18_cifar100_untargeted_adv.pth')\n",
    "model.load_state_dict(pretrained_checkpoint['state_dict'])\n",
    "model = model.to(device)\n",
    "print(\"Loaded pretrained model with clean accuracy around 50%\")\n",
    "\n",
    "# Since the model is already trained, compute class weights immediately\n",
    "print(\"Computing initial class weights...\")\n",
    "class_weights = compute_class_weights(model, val_loader)\n",
    "print(f\"Class weights range: Min={class_weights.min().item():.4f}, Max={class_weights.max().item():.4f}\")\n",
    "weighted_criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# Adjust learning rate since we're continuing from a pretrained model\n",
    "for param_group in optimizer.param_groups:\n",
    "    param_group['lr'] = param_group['lr'] * 0.5  # Reduce initial learning rate\n",
    "\n",
    "# Main training loop\n",
    "num_epochs = 200\n",
    "best_acc = 0\n",
    "best_robust_acc = 0\n",
    "epsilon = 8/255  # PGD attack strength\n",
    "alpha = 2/255    # PGD step size\n",
    "iters = 7        # Number of PGD iterations\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Update class weights every 10 epochs (after initial stabilization)\n",
    "    if epoch % 15 == 0 and epoch > 0:\n",
    "        print(\"Updating class weights...\")\n",
    "        class_weights = compute_class_weights(model, val_loader)\n",
    "        print(f\"Class weights range: Min={class_weights.min().item():.4f}, Max={class_weights.max().item():.4f}\")\n",
    "        weighted_criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    \n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    nat_correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    train_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]')\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_bar):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        # Use TRADES loss\n",
    "        loss, logits_natural, logits_adv, adv_images = trades_loss(\n",
    "            model, inputs, labels, optimizer, \n",
    "            step_size=alpha, epsilon=epsilon, \n",
    "            perturb_steps=iters, beta=8.0\n",
    "        )\n",
    "        \n",
    "        # Check for NaN or inf in loss\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            print(f\"Warning: Invalid training loss at batch {batch_idx}. Skipping update.\")\n",
    "            continue\n",
    "            \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = logits_natural.max(1)\n",
    "        total += labels.size(0)\n",
    "        nat_correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        # Calculate adversarial accuracy for the batch\n",
    "        _, adv_predicted = logits_adv.max(1)\n",
    "        adv_correct = adv_predicted.eq(labels).sum().item()\n",
    "        adv_acc = 100.0 * adv_correct / labels.size(0)\n",
    "        \n",
    "        # Update progress bar with detailed stats\n",
    "        train_bar.set_postfix({\n",
    "            'loss': f'{train_loss / (batch_idx + 1):.4f}',\n",
    "            'nat_acc': f'{100.*nat_correct/total:.2f}%',\n",
    "            'adv_acc': f'{adv_acc:.2f}%',\n",
    "            'lr': f'{scheduler.get_last_lr()[0]:.6f}'\n",
    "        })\n",
    "    \n",
    "    train_loss = train_loss / len(train_loader)\n",
    "    train_acc = 100. * nat_correct / total\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    val_bar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Val]')\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, labels) in enumerate(val_bar):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            val_bar.set_postfix({\n",
    "                'loss': f'{val_loss / (batch_idx + 1):.4f}',\n",
    "                'acc': f'{100.*val_correct/val_total:.2f}%'\n",
    "            })\n",
    "    \n",
    "    val_loss = val_loss / len(val_loader)\n",
    "    val_acc = 100. * val_correct / val_total\n",
    "    \n",
    "    # Evaluate robust accuracy on test set every 5 epochs\n",
    "    if epoch % 3 == 0 or epoch == num_epochs - 1:\n",
    "        clean_acc, robust_acc = evaluate_robustness(model, test_loader, epsilon, alpha, iters=20)\n",
    "        print(f'Clean Test Acc: {clean_acc:.2f}% | Robust Test Acc: {robust_acc:.2f}%')\n",
    "        \n",
    "        # Save model if robust accuracy improves\n",
    "        if robust_acc > best_robust_acc:\n",
    "            best_robust_acc = robust_acc\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'scheduler': scheduler.state_dict(),\n",
    "                'best_clean_acc': clean_acc,\n",
    "                'best_robust_acc': robust_acc,\n",
    "            }, 'best_robust_model.pth')\n",
    "            print(f'Robust checkpoint saved! New best robust accuracy: {best_robust_acc:.2f}%')\n",
    "    \n",
    "    # Save model if validation accuracy improves\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'scheduler': scheduler.state_dict(),\n",
    "            'best_acc': best_acc,\n",
    "        }, 'best_clean_model.pth')\n",
    "        print(f'Clean checkpoint saved! New best accuracy: {best_acc:.2f}%')\n",
    "    \n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f'Epoch: {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%')\n",
    "\n",
    "print(f\"Training completed! Best accuracy: {best_acc:.2f}%, Best robust accuracy: {best_robust_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Targeted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adversarial training loop (targeted)\n",
    "num_epochs = 200\n",
    "best_acc = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    train_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]')\n",
    "    for inputs, labels in train_bar:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        # Clean data forward pass\n",
    "        clean_outputs = model(inputs)\n",
    "        clean_loss = criterion(clean_outputs, labels)\n",
    "        \n",
    "        # Generate targeted adversarial examples\n",
    "        target_labels = (labels + 1) % 100  # Simple target: next class\n",
    "        adv_images = improved_pgd_attack(\n",
    "            model, \n",
    "            inputs, \n",
    "            labels,\n",
    "            target_labels=target_labels, \n",
    "            epsilon=8/255,  # Standard for CIFAR-100 training\n",
    "            alpha=2/255,   # Smaller for stability\n",
    "            iters=7,       # Fewer for training efficiency\n",
    "            random_start=True,\n",
    "            untargeted=False\n",
    "        )\n",
    "        \n",
    "        # Adversarial data forward pass\n",
    "        adv_outputs = model(adv_images)\n",
    "        adv_loss = criterion(adv_outputs, labels)  # Train to predict true labels\n",
    "        \n",
    "        # Combined loss (50% clean, 50% adversarial)\n",
    "        loss = 0.5 * clean_loss + 0.5 * adv_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = clean_outputs.max(1)  # Use clean outputs for accuracy\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        train_bar.set_postfix({\n",
    "            'loss': f'{train_loss/len(train_bar):.4f}',\n",
    "            'acc': f'{100.*correct/total:.2f}%',\n",
    "            'lr': f'{scheduler.get_last_lr()[0]:.6f}'\n",
    "        })\n",
    "    \n",
    "    train_loss = train_loss / len(train_loader)\n",
    "    train_acc = 100. * correct / total\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    test_bar = tqdm(test_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Test]')\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_bar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            test_bar.set_postfix({\n",
    "                'loss': f'{test_loss/len(test_bar):.4f}',\n",
    "                'acc': f'{100.*correct/total:.2f}%'\n",
    "            })\n",
    "    \n",
    "    test_loss = test_loss / len(test_loader)\n",
    "    test_acc = 100. * correct / total\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Save checkpoint if it's the best model so far\n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'scheduler': scheduler.state_dict(),\n",
    "            'best_acc': best_acc,\n",
    "        }\n",
    "        torch.save(checkpoint, 'best_resnet18_cifar100_targeted_adv.pth')\n",
    "        print(f'Checkpoint saved! New best accuracy: {best_acc:.2f}%')\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f'Epoch: {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.2f}%')\n",
    "\n",
    "print(f\"Training completed! Best accuracy: {best_acc:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
